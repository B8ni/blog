[
  {
    "objectID": "posts/2022-12-02-artificial-neurons.html",
    "href": "posts/2022-12-02-artificial-neurons.html",
    "title": "Quote from Ilya Sutskever",
    "section": "",
    "text": "Ilya Sutskever, 2022"
  },
  {
    "objectID": "posts/2022-12-02-artificial-neurons.html#reference",
    "href": "posts/2022-12-02-artificial-neurons.html#reference",
    "title": "Quote from Ilya Sutskever",
    "section": "",
    "text": "Ilya Sutskever, 2022"
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#preamble",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#preamble",
    "title": "How Random Forest Can Empower A Plant",
    "section": "Preamble",
    "text": "Preamble\nWhile the entire world is totally captured by Stable Diffusion, I’m experimenting randomly into the forest of Random Forest. Here my 2 cents after about 60+ hours of fighting against Random Forest. Actually Forrest is winning the game.\n\nWhy Predicting Scraps Boxes Weight?\nIt’s hard to fight entropy in my home.\nIt’s exponentially hard to fight entropy in a plant, in an Aluminium plant precisely.\nThe factory would gain lots of benefits when scraps are segregated, weighted and labeled in the right way1. Better the process and higher the impact on the revenue of the company (true story).\nThe weighting process is simple: take the box, put it on an industrial scale, get the weight and repeat.\nThe weighting process, although heavily based on an inductive flow, it’s not enough. The operators have still room of errors. How can I solve this problem, without spending lot’s of money? Wrong answers only: Random Forest is the answer.\nI concluded that scraps box belongs to a specific set of weights: from 0 (no box) up to 1010 KG. Simply a set of 10 boxes. It brings to a problem to solve: classification problem.\nStructured data + classification problem = RandomForestClassifier."
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#first-round",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#first-round",
    "title": "How Random Forest Can Empower A Plant",
    "section": "First Round",
    "text": "First Round\n\n\n\nModel\nDataset\n\n\n\n\nRandom Forest Classifier\nCSV file, 82k rows and 33 columns.\n\n\n\nThe dataset has been created by joying most interesting tables, adding intentionally duplicated or closely correlated columns. May increase the noise or may drive to a better prediction?\n\nPreprocessing\nSince I already worked with this data, I found a subtle feature, which is a calculated field where would create lots of troubles in production environment. net_weight is accused of Data Leakage23 so I firstly dropped it.\nObviously, Data Leakage is an issue faced later on experimentation but, IMHO, earlier you find and better it is. It’s mean you have a good understanding of data.\nVia Categorify, FillMissing, cont_cat_split and RandomSplitter functions, the data is ready to be fitted.\nfrom fastai.tabular.all import Categorify, FillMissing, cont_cat_split, RandomSplitter\ndep = \"tare_weight\"\ndf = df.drop(\"net_weight\", axis=1)\nprocs = [Categorify, FillMissing]\ncont,cat = cont_cat_split(df, 1, dep_var=dep)\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(df)\nI think cont_cat_split is a nice function to spend few seconds with. Going to its source code, I can see how genuinely the continuous and categorical variables are managed:\ndef cont_cat_split(df, max_card=20, dep_var=None):\n    \"Helper function that returns column names of cont and cat variables from given `df`.\"\n    cont_names, cat_names = [], []\n    for label in df:\n        if label in L(dep_var): continue\n        if ((pd.api.types.is_integer_dtype(df[label].dtype) and\n            df[label].unique().shape[0] &gt; max_card) or\n            pd.api.types.is_float_dtype(df[label].dtype)):\n            cont_names.append(label)\n        else: cat_names.append(label)\n    return cont_names, cat_names\nFor every column in dataframe, if it has more then 20 elements or it’s a float, appends to continuous, otherwise categorical. I’ve no experience with FastAI API, but I suppose the library is full of such elegant and simple way to manage complex data and task.\nOnce pre-processing step is completed, the dataframe is wrapped into a TabularPandas. TabularPandas is an object. It’s a simple DataFrame wrapper with transforms. Transforms are functions which organize the data in an optimal format.\n\nMachine learning models are only as good as the data that is used to train them.\n\nBetter data format, better generalization.\nfrom fastai.tabular.all import TabularPandas \nto = TabularPandas(\n    df, procs, cat, cont, \n    y_names=dep, splits=splits)\nto.train.xs.iloc[:3]\n\nNow, save and train.\nfrom fastai.tabular.all import save_pickle\nsave_pickle('to.pkl',to)\n\n\nFitting\nJeremy has developed a function which wraps RandomForestClassifier. It turns useful later to apply some tuning.\nfrom fastai.tabular.all import load_pickle\nload_pickle('to.pkl', to)\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef rf(xs, y, n_estimators=100,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\nm = rf(xs, y)\nA good error metrics, to understand what’s going on, is a simple mean_absolute_error:\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(m.predict(xs), y), mean_absolute_error(m.predict(valid_xs), valid_y)\n\nWhat’s mean_absolute_error? Going to the source code of scikit-learn, I found line which calculate MAE:\nnp.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)\nIt means: 1. calculate the delta between (y_pred - y_true) 2. take the absolute value np.abs of the whole rows axis=0 3. finally calculate the average with np.average function\nNothing to add, simple enough.\n\n\nOut Of Bag Error\nIn addition to mean_absolute_errors, I have to keep an eye on oob_score_ attribute which returns the accuracy of predictions on the residual rows scrapped during the training. Obviously higher the score better the generalization on validation set.\nm.oob_score_\n\nThere’s so much resources where explain acutely and precisely what the hell OOB is. I’m not the right person to do that. To simplify the definition I’ve impressed in my mind the following raccomandation by Jeremy: &gt; My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were not selected for that tree’s training.\n\n\nIntermediate Result\n\n\n\nRound\nOOB Score\nMAE Training set\nMAE Validation set\n\n\n\n\n1\n0.709\n47.06\n73.49\n\n\n\n47.06 and 73.49 are just numbers. But what does it mean? I have achieved, via a simple RandomForestClassifier with 100 trees (n_estimators), an average of: - 47.06 KG of error on training set - 73.49 KG of error on validation set\nAnd an accuracy of 0.709 on the residual data not included in the fitting step.\nIt’s clear there are multiple goals to try to achieve. A good trade off could be the following chain: small_enough_error &gt; stability &gt; maintainability\nThe next steps I’m going to walk, aims to improve the above chain."
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#second-round",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#second-round",
    "title": "How Random Forest Can Empower A Plant",
    "section": "Second Round",
    "text": "Second Round\nRandomForest is composed by multiples DecisionTrees. DecisionTrees are highly interpretable, so it’s time to investigate the data: 1. analyzing the most important columns, AKA feature_importances_ 2. analyzing the prediction behavior for each row, AKA treeinterpreter 3. finding redundant columns, AKA cluster_columns 4. analyzing prediction confidence of the model, AKA std of each tree prediction 5. analyzing the relationship between independent variables and dependent variable, AKA partial_dependece 6. finding out of domain data, AKA extrapolation problem 7. analyzing where most wrong predictions go, AKA confusion_matrix\n\nFeature Importances\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\nfi = rf_feat_importance(m, xs)\nfi[:5]\n\nAccording to the above table: - the box weight prediction is mainly influenced by weight itself4. Sounds reasonable; - id_machine, in other words the machine which generates scraps, is the second most important indicator of box weight prediction. Sounds reasonable as well; - id_machine_article_description is the combination between id_machine, article and description_machine, where article is the thickness range of scarps (Ex.: from 0.5mm to 0.25mm); - percentage of id and timestamp is too similar. Maybe, periodically, I can expect a specific type of scraps? - code_machine is the short name of machine; - last_name, the operator, contributes to the box weight prediction as well. Maybe some operators are more diligent then others? - description_machine is extended name of machine;\nEverything sounds reasonable so it seems I’ve discovered nothing so useful.\nLet’s visualize feature_importances_ columns.\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n\nNow let’s remove from training and validation sets features which tend to 0 .\nfi = fi[fi[\"imp\"] &lt; 0.002]\n\nfiltered_xs = xs.drop(fi[\"cols\"], axis=1)\nfiltered_valid_xs = valid_xs.drop(fi[\"cols\"], axis=1)\nThen fitting again the model and check the error rate (mean_absolute_error and oob_score_).\nm = rf(filtered_xs, y)\nmean_absolute_error(m.predict(filtered_xs), y), mean_absolute_error(m.predict(filtered_valid_xs), valid_y)\n\nm.oob_score_\n\nHas been achieved few improvements: - mean_absolute_error on training set is smaller: from 47.06 to 46.78; - mean_absolute_error on validation set is smaller: from 73.49 to 73.47; - oob_score_ stable: from 0.709 to 0.708 - features reduced: from 33 to 25.\nNow, let’s hunt redundant features.\n\n\nRedundant Features\n# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ndef corr_filter(x: pd.DataFrame, bound: float):\n    corr = x.corr()\n    x_filtered = corr[((corr &gt;= bound) | (corr &lt;= -bound)) & (corr !=1.000)]\n    x_flattened = x_flattened.unstack().sort_values().drop_duplicates()\n    return x_flattened\n\ncorr_filter(filtered_xs, .8)\n Giving a threshold of 0.8, function will return set of elements highly correlated with a score from 0.8 to 0.9999.\nFor a better understanding, worth to visualize them.\n# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nxs_corr = filtered_xs.corr()\ncompressed_xs = xs_corr[((xs_corr &gt;= .5) | (xs_corr &lt;= -.5)) & (xs_corr !=1.000)]\nplt.figure(figsize=(30,10))\nsn.heatmap(compressed_xs, annot=True, cmap=\"Reds\")\nplt.show()\n\nAn alternative to heatmap is the helper function cluster_columns which implement a dendrogram chart.\n# https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\nNow, iteratively remove every closely correlated feature and calculate oob_score_. This task is performed by get_oob function:\ndef get_oob(df):\n    m = RandomForestClassifier(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\nget_oob(filtered_xs)\n\nto_drop = [\"id\", \"timestamp\", \"slim_alloy\", \"id_alloy\", \"pairing_alloy\",\n           \"international_alloy\", \"id_user\", \"address\",\n           \"location_name\", \"article_min_tickness\", \"article_max_tickness_na\"]\n{c:get_oob(filtered_xs.drop(c, axis=1)) for c in to_drop}\n\nGoing to remove only features with higher score.\nto_drop = [\"timestamp\", \"id_alloy\", \"id_user\", \"address\", \"article_min_tickness\"]\nfiltered_xs = filtered_xs.drop(to_drop, axis=1)\nfiltered_valid_xs = filtered_valid_xs.drop(to_drop, axis=1)\nm = rf(filtered_xs, y)\nmean_absolute_error(m.predict(filtered_xs), y), \nmean_absolute_error(m.predict(filtered_valid_xs), valid_y)\n\nm.oob_score_\n\n\n\nIntermediate Result\n\n\n\nRound\nOOB Score\nMAE Training set\nMAE Validation set\n\n\n\n\n1\n0.709\n47.06\n73.49\n\n\n2\n0.708\n49.37\n74.09\n\n\n\nNot much worse than the model with all the fields. I’ve reduced some more columns (from 25 to 20) and kept stable oob_score_. Removing redundant features help to prevent overfitting."
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#third-round",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#third-round",
    "title": "How Random Forest Can Empower A Plant",
    "section": "Third Round",
    "text": "Third Round\nAs showed by Jeremy, Random Forest can suffer of Extrapolation problem (:open_mouth:). \nIt means, in this case, predictions are too low with new data.\n\nRemember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low.\n\nFor this reason I’ve to make sure validation set does not contain out-of-domain data.\n\nOut-of-Domain Data\nHow to understand if the data is distributed quite properly on training set and validation set?\ndf_dom = pd.concat([filtered_xs, filtered_valid_xs])\nis_valid = np.array([0]*len(filtered_xs) + [1]*len(filtered_valid_xs))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:15]\n\nNow, for each feature which vary a lot from training set and validation set, try to drop and check mean_absolute_error. Finally, select those that keep improving the model.\nprint('orig', mean_absolute_error(m.predict(filtered_valid_xs), valid_y))\n\nfor c in ('id','weight', 'international_alloy', 'slim_alloy',\n          'pairing_alloy', 'id_machine_article_description', 'location_name', \"last_name\"):\n    m = rf(filtered_xs.drop(c,axis=1), y)\n    print(c, mean_absolute_error(m.predict(filtered_valid_xs.drop(c,axis=1)), valid_y))\n\nLet’s drop only slim_alloy.\nto_drop = ['slim_alloy']\n\nxs_final = filtered_xs.drop(to_drop, axis=1)\nvalid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n\nm = rf(xs_final, y)\nmean_absolute_error(m.predict(valid_xs), valid_y)\n\nKeep checking out of bag error:\nm.oob_score_\n\n\n\nIntermediate Result\n\n\n\nRound\nOOB Score\nMAE Training set\nMAE Validation set\n\n\n\n\n1\n0.709\n47.06\n73.49\n\n\n2\n0.708\n49.37\n74.09\n\n\n3\n0.707\n\n73.98\n\n\n\nGood news, working on out-of-domain data has improved mean_absolute_error and stabilize oob_score_: - from 74.09 KG to 73.98 KG, validation set; - from 0.708 to 0.707, oob_score_.\nWhat I have achieved so far are only small improvements. Looking at a simple chart which plots the delta between real value and prediction, I can see there’s still lot of room of improvement. \nSome datapoints are consistently predicted wrong (dots at about -900/-1000 and about 900/1000). Other visual tools like Confusion matrix , prediction confidence, treeinterpreter can help to analyze those behaviors."
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#final-round",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#final-round",
    "title": "How Random Forest Can Empower A Plant",
    "section": "Final Round",
    "text": "Final Round\nBefore any hyper-mega-super-giga tuning, my last attempt is to remove older data.\nWhy? The application which manages the weighting/labeling process of scraps5 has been released about 2 years ago. Wouldn’t surprise me if I found some strange data points, especially during first period of usage where operators were not comfortable yet with the system.\nRe-processing whole steps removing older 12k datapoints, seems to have generated a better baseline. \nThere’s still miss-classification at around -900/-1000 and 900/1000, it’s worth investigating. However it’s evident has been achieved an improvement.\n\nResult\n\n\n\nRound\nOOB Score\nMAE Training set\nMAE Validation set\n\n\n\n\n1\n0.709\n47.06\n73.49\n\n\n2\n0.708\n49.37\n74.09\n\n\n3\n0.707\n\n73.98\n\n\n4\n0.714\n\n72.17\n\n\n\nI think as baseline model is good entry level: fast to fit, easily interpretable and quite stable.\nWhat I want to point out is that this small experimentation has been possible with few KBs of data, a laptop and a mediocre baseline model. An empowered model version, will avoid to spend several thousand dollars on revamping of machines!"
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#whats-next",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#whats-next",
    "title": "How Random Forest Can Empower A Plant",
    "section": "What’s Next?",
    "text": "What’s Next?\nOnce created a baseline model on simplified dataset, it’s time to make a decision: - creating a NN model - or working on Radom Forest tuning - or switching to XGBoost model\n\n…roughly 80% of consequences come from 20% of causes…\n\nAs per Pareto principle, it means, for me, to try to leverage and get as good result as soon as possible while keeping at the minimum the effort.\nSo next steps: - I will implement a Neural Network model - then I’ll combine NN with Random Forest - the ensembles will work in parallel with a Computer Vision model which will try to classify the same problem (a box of scraps).\nAll those stuffs are aimed to develop a system where departments are notified every time the prediction of models are too different from what’s happening during the weighting process.\nI’ve in mind already the application name: Box ClassifAI.\nKeep the scraps errors lower and push higher the revenue. That’s it."
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#open-points",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#open-points",
    "title": "How Random Forest Can Empower A Plant",
    "section": "Open Points",
    "text": "Open Points\n\nWhat happens if I play with categorical and continuous variables? Can them affect the prediction?\nPlotting dendogram and removing most correlated columns. Does it change the prediction? Are columns the same?\nWhy is confidence of prediction totally wrong when the deviation reach value 100? Why is prediction not so bad with greater value?\nPartial dependency plots for multi-class-classifiers?\nCould improve Random Forest model with additional information like weather data?\nWhat’s happen if I convert the problem into a regression one? May the result improve?\n\nIf you have any suggestions, recommendations, or corrections please reach out to me."
  },
  {
    "objectID": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#footnotes",
    "href": "posts/aluminium-scrap-box/2022-10-26-aluminium-scraps-box-weight-random-forest-post.html#footnotes",
    "title": "How Random Forest Can Empower A Plant",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have developed some tools to speed up and managing the process of the Aluminium scarps weighting↩︎\nA gentle introduction to Data Leakage can be found on Kaggle course↩︎\nA formal introduction to Data Leakage can be found on Leakage in Data Mining paper↩︎\nWeight - Box Weight = Net Weight↩︎\nWe have developed some tools to speed up and managing the process of the Aluminium scarps weighting↩︎"
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html",
    "title": "The Random Forest Guy",
    "section": "",
    "text": "Days passed from my first Random Forest practical experiment, where I was attempting to predict the weight of an Aluminium Scarp Box.\nSpending days going deeper on Random Forest, here you can find a revisioned and hope improved version of the previous one post.\nShort learning cycle suggested me, gradually, what’s matter the most.\nFigure out the metrics properly.\nSame tip and trick came from Thakur book where he underlines, before any kind of splitting: understand the data and implement the right metric.\nTarget drives metric, therefore undestanding deeply the target will return the right metric.\n\n\nInitially the problem to solve included 681 classes. Now I’ve kept only the 11 most common.\nPreviously I was using the wrong metric, today I switched to AUC ROC metric where it’s mainly used on multi class classification problem.\nSo, but what’s the target? A multi class classification problem with imbalanced data. It took me a while but worth it.\nWait, imbalanced what? I don’t know yet. Let’s dig into unbalanced data another day."
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#scrap-box-dataset",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#scrap-box-dataset",
    "title": "The Random Forest Guy",
    "section": "",
    "text": "Days passed from my first Random Forest practical experiment, where I was attempting to predict the weight of an Aluminium Scarp Box.\nSpending days going deeper on Random Forest, here you can find a revisioned and hope improved version of the previous one post.\nShort learning cycle suggested me, gradually, what’s matter the most.\nFigure out the metrics properly.\nSame tip and trick came from Thakur book where he underlines, before any kind of splitting: understand the data and implement the right metric.\nTarget drives metric, therefore undestanding deeply the target will return the right metric.\n\n\nInitially the problem to solve included 681 classes. Now I’ve kept only the 11 most common.\nPreviously I was using the wrong metric, today I switched to AUC ROC metric where it’s mainly used on multi class classification problem.\nSo, but what’s the target? A multi class classification problem with imbalanced data. It took me a while but worth it.\nWait, imbalanced what? I don’t know yet. Let’s dig into unbalanced data another day."
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#explore-the-dataset",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#explore-the-dataset",
    "title": "The Random Forest Guy",
    "section": "Explore the Dataset",
    "text": "Explore the Dataset\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"scraps/scrap_202210181239.csv\")\ndf.shape\ndf[\"tare_weight\"].nunique()\ndf[\"tare_weight\"].value_counts().head(11)\ntop_classes = df[\"tare_weight\"].value_counts().head(11)\n(df.shape[0]-top_classes.sum())/df.shape[0] *100\nIn my case I want to reduce the target spectrum. From 681 classes to 11 classes. This target reduction impacts the dataset by 4.47% of size. 670 classes are the result of inappropriate software usage. I’m pretty confident the current inserts are happening mostly right.\ntop_classes[\"top_classes\"] = top_classes.index\ndf = df[df['tare_weight'].isin(top_classes[\"top_classes\"])]\ndf.shape\n82388 - 78708\nRemoved 3680 rows which meet the 670 surplus classes: a bit cut for a big up.\nLet’s see features and target correlation with pairplot method.\nimport seaborn as sns\n# df_2 = df_2[df_2[\"weight\"] &lt;= 3500]\nsns.pairplot(df[:50], hue=\"tare_weight\")\nI don’t see any strong linear correlation (except fews which are duplicated features). It suggests Random Forest, thanks to its ability to work uninformative features, would take advantage of the dataset form."
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#data-preprocessing",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#data-preprocessing",
    "title": "The Random Forest Guy",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nfrom fastai.tabular.all import Categorify, FillMissing, cont_cat_split, RandomSplitter\n\ndep = \"tare_weight\"\n\ndf = df.drop(\"net_weight\", axis=1)\n\nprocs = [Categorify, FillMissing]\ndf = df.rename(columns={\"max_tickness.1\": \"article_max_tickness\",\n                        \"min_tickness.1\": \"article_min_tickness\",\n                        \"max_tickness\": \"alloy_max_tickness\",\n                        \"min_tickness\": \"alloy_min_tickness\",\n                        \"name\": \"location_name\"})\ncont,cat = cont_cat_split(df, 1, dep_var=dep)\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(df)\nfrom fastai.tabular.all import TabularPandas \nto = TabularPandas(\n    df, procs, cat, cont, \n    y_names=dep, splits=splits)\nto.train.xs.iloc[:3]\nlen(to.train),len(to.valid)    \nfrom fastai.tabular.all import save_pickle\nsave_pickle('to.pkl',to)\nfrom fastai.tabular.all import load_pickle\nto = load_pickle('to.pkl')\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\ndef ovr_rf(xs, y, n_estimators=40,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True)).fit(xs, y)\nHere I’ve simply re-adapted a Jeremy function to work with One-Versus-Rest pipeline. I’m improving my buzzy worlds man!\nm  = ovr_rf(xs, y)\npred_prob = m.predict_proba(valid_xs)\npred_prob\nActually I’m not using the classic predict() method. pred_prob is an array - generated by predict_proba() method - which contains classes probabilities. See also sklearn source code.\n\nROC Curve\nNow it’s time to analyze our performance with a different metric: AUC ROC.\nFirst encoding all classes then binirize and finally plot them.\n#Lets encode target labels (y) with values between 0 and n_classes-1.\n#We will use the LabelEncoder to do this. \nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\nlabel_encoder.fit(valid_y)\ntransfomerd_valid_y=label_encoder.transform(valid_y)\nclasses=label_encoder.classes_\nfrom sklearn.preprocessing import label_binarize\n#binarize the y_values\nplt.figure(figsize = (15, 10))\n\ny_test_binarized=label_binarize(valid_y,classes=np.unique(valid_y))\n\n# roc curve for classes\nfpr = {}\ntpr = {}\nthresh ={}\nroc_auc = dict()\n\nn_class = classes.shape[0]\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred_prob[:,i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # plotting    \n    plt.plot(fpr[i], tpr[i], linestyle='--', \n             label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n    \n\nplt.plot([0,1],[0,1],'b--')\nplt.xlim([0,1])\nplt.ylim([0,1.05])\nplt.title('Multiclass ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='lower right')\nplt.show()\navg_roc_auc = pd.Series(roc_auc)\navg_roc_auc.mean()\nAn average of 94% of being right is really good. Only 750 box is mainly miss-classified, with a 83%."
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#feature-selection",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#feature-selection",
    "title": "The Random Forest Guy",
    "section": "Feature Selection",
    "text": "Feature Selection\nFeature selection starts from feature_importances_. I’ve adapted Jeremy method to work with multi class model.\n\nFeature Importances\ndef rf_feat_importance(m, df, i):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.estimators_[i].feature_importances_}\n                       ).sort_values('imp', ascending=False)\nEvery class have its own feature importances so I have to compress everything in array and remove the last one. I’ve implemented a simple concat and mean.\ndf_all = pd.DataFrame()\nfor i in range(df[\"tare_weight\"].nunique()):\n    df_all = pd.concat([df_all, rf_feat_importance(m, xs, i)])\ncols = df_all[\"cols\"].sort_index().unique()\ndf_all = df_all.groupby(df_all.index).mean()\ndf_all[\"cols\"] = cols\ndf_all = df_all.sort_values('imp', ascending=False)\nFinally plotting averaged feature importances of the whole classes.\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(df_all[:30]);\nLet’s remove less significant ones.\ndf_all[df_all[\"imp\"] &gt;= 0.002]\nfi = df_all[df_all[\"imp\"] &lt; 0.002]\n\nfiltered_xs = xs.drop(fi[\"cols\"], axis=1)\nfiltered_valid_xs = valid_xs.drop(fi[\"cols\"], axis=1)\nfiltered_xs.shape, filtered_valid_xs.shape\nm = ovr_rf(filtered_xs, y)\npred_prob = m.predict_proba(filtered_valid_xs)\ndef roc_plot(classes):\n    plt.figure(figsize = (15, 10))\n\n    y_test_binarized=label_binarize(valid_y,classes=np.unique(valid_y))\n\n    # roc curve for classes\n    fpr = {}\n    tpr = {}\n    thresh ={}\n    roc_auc = dict()\n\n    n_class = classes.shape[0]\n\n    for i in range(n_class):    \n        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred_prob[:,i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # plotting    \n        plt.plot(fpr[i], tpr[i], linestyle='--', \n                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n\n\n    plt.plot([0,1],[0,1],'b--')\n    plt.xlim([0,1])\n    plt.ylim([0,1.05])\n    plt.title('Multiclass ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive rate')\n    plt.legend(loc='lower right')\n    plt.show()\nroc_plot(classes)\ndef avg_roc_auc(pred_prob):\n    return pd.Series(roc_auc_classes(pred_prob)).mean()  \navg_roc_auc(pred_prob)\nWith just removing the less important ones, the model has improved by few decimals.\nfrom fastai.tabular.all import save_pickle\nsave_pickle('filtered_xs.pkl',filtered_xs)\nsave_pickle('filtered_valid_xs.pkl',filtered_valid_xs)\nfiltered_xs = load_pickle('filtered_xs.pkl')\nfiltered_valid_xs = load_pickle('filtered_valid_xs.pkl')\n\n\nFeatures Correlation\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nxs_corr = filtered_xs.corr()\ncompressed_xs = xs_corr[((xs_corr &gt;= .5) | (xs_corr &lt;= -.5)) & (xs_corr !=1.000)]\nplt.figure(figsize=(30,10))\nsn.heatmap(compressed_xs, annot=True, cmap=\"Reds\")\nplt.show()\ndef corrFilter(x: pd.DataFrame, bound: float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr &gt;= bound) | (xCorr &lt;= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\n\ncorrFilter(filtered_xs, .65)\ndef oob_estimators(filtered_xs):\n    m = ovr_rf(filtered_xs, y)\n    return [m.estimators_[i].oob_score_ for i in range (df[\"tare_weight\"].nunique())]\nSince I’m working with a dataset with 11 classes, it’s essential to evaluate for each class relative Out-of-Bag score. So the goal is to remove closely correlated features which keep stagnant or improve the OOB score.\noob_estimators(filtered_xs)\nto_drop = [\"id\", \"timestamp\", \"slim_alloy\", \"id_alloy\", \"pairing_alloy\",\n           \"international_alloy\", \"id_user\", \"address\",\n           \"location_name\", \"article_min_tickness\", \"article_max_tickness_na\"]\n{c:oob_estimators(filtered_xs.drop(c, axis=1)) for c in to_drop}\nThe features belongs to to_drop list with an average of OOB score higher, will be dropped.\nto_drop = [\"id\", \"pairing_alloy\", \"id_alloy\",\n           \"article_max_tickness_na\", \"location_name\", \"article_max_tickness_na\"]\nfiltered_xs = filtered_xs.drop(to_drop, axis=1)\nfiltered_valid_xs = filtered_valid_xs.drop(to_drop, axis=1)\nfiltered_valid_xs.shape, filtered_xs.shape\nm = ovr_rf(filtered_xs, y)\npred_prob = m.predict_proba(filtered_valid_xs)\nroc_plot(pred_prob)\navg_roc_auc(pred_prob)\nObtaining 94.5% AUC ROC score while keeping OOB score higher is a good achievement. Breakpoint saved.\nsave_pickle('filtered_xs.pkl',filtered_xs)\nsave_pickle('filtered_valid_xs.pkl',filtered_valid_xs)\nfiltered_xs = load_pickle('filtered_xs.pkl')\nfiltered_valid_xs = load_pickle('filtered_valid_xs.pkl')"
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#baseline-result",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#baseline-result",
    "title": "The Random Forest Guy",
    "section": "Baseline Result",
    "text": "Baseline Result\nNow it’s time to fix Out of Domain Data to minimize overfitting.\n\nOut of Domain Data\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\ndf_dom = pd.concat([filtered_xs, filtered_valid_xs])\nis_valid = np.array([0]*len(filtered_xs) + [1]*len(filtered_valid_xs))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:15]\nfor c in ('timestamp', 'weight', 'slim_alloy', \n          'international_alloy', 'id_machine_article_description',\n          'id_idp_user', 'last_name', 'id_machine', 'slim_number',\n          'first_name', 'code_machine', 'description_machine'):\n    m = ovr_rf(filtered_xs.drop(c,axis=1), y)\n    pred_prob = m.predict_proba(filtered_valid_xs.drop(c,axis=1))\n    print(c, avg_roc_auc(pred_prob))\nto_drop = ['international_alloy', 'last_name', 'id_machine', 'slim_number', 'description_machine']\nxs_final = filtered_xs.drop(to_drop, axis=1)\nvalid_xs = filtered_valid_xs.drop(to_drop, axis=1)\nxs_final.shape, valid_xs.shape\nm = ovr_rf(filtered_xs, y)\npred_prob = m.predict_proba(filtered_valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(m)\nEverything ended with less features (15) and higher score both AUC ROC and OOB.\nsave_pickle('final_xs.pkl',xs_final)\nsave_pickle('final_valid_xs.pkl',valid_xs)\n\n\nHyperparameter Tuning\nBefore the game end I’ll try some hypertuning.\nxs_final = load_pickle('final_xs.pkl')\nvalid_xs = load_pickle('final_valid_xs.pkl')\nm.get_params()\nfrom sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 4)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'estimator__n_estimators': n_estimators,\n               'estimator__max_features': max_features,\n               'estimator__max_depth': max_depth,\n               'estimator__min_samples_split': min_samples_split,\n               'estimator__min_samples_leaf': min_samples_leaf,\n               'estimator__bootstrap': bootstrap}\nrandom_grid\nfrom sklearn.model_selection import ShuffleSplit\nsp = ShuffleSplit(n_splits=2, test_size=.25, random_state=42)\nrf.get_params().keys()\nfrom sklearn.ensemble import RandomForestClassifier \n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = OneVsRestClassifier(RandomForestClassifier(oob_score=True))\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = sp, verbose=2, random_state=42, n_jobs = 3)# Fit the random search model\nrf_random.fit(xs_final, y)\nrf_random.best_params_\nfrom sklearn.metrics import accuracy_score\nbest_model = rf_random.best_estimator_\npred_prob = best_model.predict_proba(valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(best_model)\nNow narrowing the range and trying to gain lil decimals.\nfrom sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\nn_estimators = [50, 100, 150]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [30, 50, 100]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10, 20]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True]# Create the random grid\nrandom_grid = {'estimator__n_estimators': n_estimators,\n               'estimator__max_features': max_features,\n               'estimator__max_depth': max_depth,\n               'estimator__min_samples_split': min_samples_split,\n               'estimator__min_samples_leaf': min_samples_leaf,\n               'estimator__bootstrap': bootstrap}\nrandom_grid\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = OneVsRestClassifier(RandomForestClassifier(oob_score=True))\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = sp, verbose=2, random_state=42, n_jobs = 3)# Fit the random search model\nrf_random.fit(xs_final, y)\nrf_random.best_estimator_\nnarrowed_model = rf_random.best_estimator_\npred_prob = narrowed_model.predict_proba(valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(narrowed_model)\nFrom 94% to almost 94.7% is the final score. OOB stable on 95.3% range."
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#conclusion",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#conclusion",
    "title": "The Random Forest Guy",
    "section": "Conclusion",
    "text": "Conclusion\nMiss-classifying the tare weight (Aluminium scarp box) is expensive causing damage to the company (less revenue) and environment (re-melting Aluminium).\nScoring a 94.7% of predicting right is a great baseline. Sure less scraps will be wasted."
  },
  {
    "objectID": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#further-work",
    "href": "posts/random-forest-guy/2022-11-17-the-random-forest-guy.html#further-work",
    "title": "The Random Forest Guy",
    "section": "Further Work",
    "text": "Further Work\n\nDevelop service which host the model.\nHow reacts the model if I remove duplicated rows? Do it.\nI know the dataset is imbalanced. Implement it.\nCompare the result with deep learning tabular model.\nCompare the result with XGBoost model.\nUsing same method to classify Aluminium alloys."
  },
  {
    "objectID": "posts/confusion-matrix/2022-11-15-confusion-matrix.html",
    "href": "posts/confusion-matrix/2022-11-15-confusion-matrix.html",
    "title": "Confusion Matrix",
    "section": "",
    "text": "Confusion Matrix, is a matrix and… we all agree.\nConfusion Matrix can answer to the questions: - How is going the prediction? - Which one I constantly missed?\nConfusion Matrix is composed by following components: \nWhat’s TP? - True Positive: it’s what the model have predicted right. On my current work I’m trying to predict the weight of a box (about 11 classes). The box weight 750? Yes, it falls into TP recipient. If not, it falls into FP.\nWhat’s FP? - False Positive: it’s what the model have predicted wrong. The model thought the box weight 750 KG but it isn’t. Wrong prediction.\nWhat’s TN: - True Negative: the model understood that box weight isn’t 750 KG. It falls into TN recipient. If not, it falls into FN.\nWhat’s FN: - False Negative: the model predicted not 750 KG and was wrong. It falls into FN recipient. it’s the opposite behavior of FP. Someone would say: invert. always invert"
  },
  {
    "objectID": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#whats-confusion-matrix",
    "href": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#whats-confusion-matrix",
    "title": "Confusion Matrix",
    "section": "",
    "text": "Confusion Matrix, is a matrix and… we all agree.\nConfusion Matrix can answer to the questions: - How is going the prediction? - Which one I constantly missed?\nConfusion Matrix is composed by following components: \nWhat’s TP? - True Positive: it’s what the model have predicted right. On my current work I’m trying to predict the weight of a box (about 11 classes). The box weight 750? Yes, it falls into TP recipient. If not, it falls into FP.\nWhat’s FP? - False Positive: it’s what the model have predicted wrong. The model thought the box weight 750 KG but it isn’t. Wrong prediction.\nWhat’s TN: - True Negative: the model understood that box weight isn’t 750 KG. It falls into TN recipient. If not, it falls into FN.\nWhat’s FN: - False Negative: the model predicted not 750 KG and was wrong. It falls into FN recipient. it’s the opposite behavior of FP. Someone would say: invert. always invert"
  },
  {
    "objectID": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#when-confusion-matrix",
    "href": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#when-confusion-matrix",
    "title": "Confusion Matrix",
    "section": "When Confusion Matrix",
    "text": "When Confusion Matrix\nThis kind of matrix is applicable only on Classification Problem."
  },
  {
    "objectID": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#why-confusion-matrix",
    "href": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#why-confusion-matrix",
    "title": "Confusion Matrix",
    "section": "Why Confusion Matrix",
    "text": "Why Confusion Matrix\nOn top of Confusion Matrix have been developer lots of metrics. : - Accuracy - Precision - Recall - F-Score - AUC ROC"
  },
  {
    "objectID": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#implementation",
    "href": "posts/confusion-matrix/2022-11-15-confusion-matrix.html#implementation",
    "title": "Confusion Matrix",
    "section": "Implementation",
    "text": "Implementation\nIf you have any suggestions, recommendations, or corrections please reach out to me."
  },
  {
    "objectID": "posts/2023-01-24-reading-book-2023.html",
    "href": "posts/2023-01-24-reading-book-2023.html",
    "title": "Goal n. 8 - Read 12 books",
    "section": "",
    "text": "Ken Follet - Eye of the Needle, 2023/01\n\nIf you have any suggestions, recommendations, or corrections please reach out to me."
  },
  {
    "objectID": "posts/2022-12-12-neural-network.html",
    "href": "posts/2022-12-12-neural-network.html",
    "title": "Neural Network",
    "section": "",
    "text": "Neural Network is a mathematical function.\nThis mathematical function is based on inputs and parameters.\nInputs and parameters are multiplied and added them up. Negative values are set to zero.\nThese operations are repeated untill the error of prediction is minimized."
  },
  {
    "objectID": "posts/2022-12-12-neural-network.html#one-liner-definition",
    "href": "posts/2022-12-12-neural-network.html#one-liner-definition",
    "title": "Neural Network",
    "section": "",
    "text": "Neural Network is a mathematical function.\nThis mathematical function is based on inputs and parameters.\nInputs and parameters are multiplied and added them up. Negative values are set to zero.\nThese operations are repeated untill the error of prediction is minimized."
  },
  {
    "objectID": "posts/2022-12-12-neural-network.html#motivation",
    "href": "posts/2022-12-12-neural-network.html#motivation",
    "title": "Neural Network",
    "section": "Motivation",
    "text": "Motivation\nThese 3 simple steps are the foundation of any deep learning model.\nImplicitally they touch most important parts of NN: 1. Inputs and parameters are multiplied and added them up =&gt; Matrix multiplication; 2. Negative values are set to zero =&gt; Rectified Linear function; 3. Operations repeated untill error of prediction is minimized =&gt; Gradient descent on Loss function.\nThe most complex deep learning model is built on these foundamentals. Deeply understanding them will help to breaking every complex model out."
  },
  {
    "objectID": "posts/2022-12-12-neural-network.html#implementation",
    "href": "posts/2022-12-12-neural-network.html#implementation",
    "title": "Neural Network",
    "section": "Implementation",
    "text": "Implementation\ndef f(x): return  3*x**2 + 2*x + 1\ndef quad(a, b, c, x): return a*x**2 + b*x + c\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\nimport numpy as np\nimport torch\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\nfrom functools import partial\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)\ndef mae(pred, actual): return torch.abs(pred-actual).mean()\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\nimport torch\nabc = torch.Tensor([1.1, 1.1, 1.1])\nabc.requires_grad_()\n\nloss = quad_mae(abc)\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\nstep=0; loss=2.42\nstep=1; loss=2.40\nstep=2; loss=2.36\nstep=3; loss=2.30\nstep=4; loss=2.21\nstep=5; loss=2.11\nstep=6; loss=1.98\nstep=7; loss=1.85\nstep=8; loss=1.72\nstep=9; loss=1.58"
  },
  {
    "objectID": "posts/2022-12-12-neural-network.html#take-away",
    "href": "posts/2022-12-12-neural-network.html#take-away",
    "title": "Neural Network",
    "section": "Take Away",
    "text": "Take Away\n\nMatrix multiplication is the key to quickly calculate multuplication and addition of inputs and parameters.\nGradient descent is the tool used to understand how to minimize the loss function, since loss function composed by parameters abc.\nRectified Linear function, known as ReLU, is a linear function which takes input and the ouput is equals to the input. If the input is negative the output is zero. It’s defined as: \\(f(x) = max(0, x)\\)\n\n\nFurther Work\n\nAnkify:\n\nMatrix multiplication\nGradient descent\nReLU\nEnd to end GD which every DL model is based on\n\nDevelop a Neural Network from scratch"
  },
  {
    "objectID": "posts/2022-12-12-neural-network.html#references",
    "href": "posts/2022-12-12-neural-network.html#references",
    "title": "Neural Network",
    "section": "References",
    "text": "References\n\nNeural net foundations - Jeremy Howard, 2022\nHow does a neural net really work? - Jeremy Howard, 2022\nChatting with ChatGPT"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "There’s nothing to say about me. I’m another human being, like you."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes and ideas (in many cases, incomplete or broken)",
    "section": "",
    "text": "Be Like a Circus Performer\n\n\n\n\n\nEmbrace the Circus Performer Mentality.\n\n\n\n\n\n\nAug 12, 2023\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nGoal n. 8 - Read 12 books\n\n\n\n\n\nUpdates on Goal n.8 - Reading 12 books.\n\n\n\n\n\n\nJan 24, 2023\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\n2023 Goals\n\n\n\n\n\nMy 20 tasks to achieve in 2023.\n\n\n\n\n\n\nDec 28, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nQuote from Bottoni\n\n\n\n\n\nDon’t be afraid to build.\n\n\n\n\n\n\nDec 13, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nQuote from Karpathy\n\n\n\n\n\nAll Fallen Empires begin the game in Sleeping status. Despite their complete development and immense power, the Fallen Empires will remain passive, staying within their borders and taking no action unless provoked.\n\n\n\n\n\n\nDec 13, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nNeural Network\n\n\n\n\n\nWikibot Neural Network.\n\n\n\n\n\n\nDec 12, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nQuote from Ilya Sutskever\n\n\n\n\n\nDeep learning is based on the audacious conjecture that biological neurons and artificial neurons are not that different. Its success to date is evidence for this belief.\n\n\n\n\n\n\nDec 2, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nThe Random Forest Guy\n\n\n\n\n\nAluminium Scrap Box Classification.\n\n\n\n\n\n\nNov 17, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nConfusion Matrix\n\n\n\n\n\nWikibot Confusion Matrix.\n\n\n\n\n\n\nNov 15, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nHow Random Forest Can Empower A Plant\n\n\n\n\n\nPractical use case of Random Forest in Industrial environment.\n\n\n\n\n\n\nOct 26, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\n  \n\n\n\n\nLearning AI And Failing Miserably\n\n\n\n\n\nA minimal guide of failing learning AI.\n\n\n\n\n\n\nAug 7, 2022\n\n\nFrancesco Bottoni\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-13-dont-afraid-to-build.html",
    "href": "posts/2022-12-13-dont-afraid-to-build.html",
    "title": "Quote from Bottoni",
    "section": "",
    "text": "Bottoni, 2022"
  },
  {
    "objectID": "posts/2022-12-13-dont-afraid-to-build.html#reference",
    "href": "posts/2022-12-13-dont-afraid-to-build.html#reference",
    "title": "Quote from Bottoni",
    "section": "",
    "text": "Bottoni, 2022"
  },
  {
    "objectID": "posts/2022-12-28-goals-of-2023.html",
    "href": "posts/2022-12-28-goals-of-2023.html",
    "title": "2023 Goals",
    "section": "",
    "text": "1000h on learning ML\n\ncomplete FastAI Part 1 course\ncomplete FastAI Part 2 course\ndeploy course\nrelease 4 ML software in production\n\ndetails: release a software each 3 months\n\nuse seriously ChatGPT and or CoPilot\n\nwriting 52 articles\n\ndetails: write an article per week on the weekend\n\nuse at least 300 days Anki\nbecome MLSE this year\n\ndetails: change job description or change job\n\nearning 60k per year\nsmall but stable revenue with side project\n1.48min/100m swimming\nread 12 books\nmeditate 365 days\nyoga 250 days\nmorning gratitude and appreciation 365 days\nstart teaching logic to my son\nstart playing piano\nreach 22k of cash on Sep 2023 to pay dad’s debit\nparticipate at festivaldellamente\nparticipate at versilianafestival\nparticipate at 1 AI conference\nparticipate at 1 SWE conference\nno sugar\n1800 minutes of audiobook/podcast\n\ndetails: 30 min per working-day while driving\n\n\nIf you have any suggestions, recommendations, or corrections please reach out to me."
  },
  {
    "objectID": "posts/2023-08-12-circus-performer.html",
    "href": "posts/2023-08-12-circus-performer.html",
    "title": "Be Like a Circus Performer",
    "section": "",
    "text": "Running a startup successfully often requires adopting the mindset of a circus performer. Imagine everyone coming to watch a show, where the same performer who wows the audience with a somersault is the one who took your ticket and ushered you to your seat.\nBeing a generalist is often the best approach — and sometimes, the only road to success.\nIf the term generalist doesn’t resonate with you, think of yourself as a versatile circus performer.\nEmbrace the idea of tearing up the ticket, giving a stellar performance, and then dismantling your equipment.\nYes, you have to be ready to step outside your comfort zone, and at times, even your passion, if you truly want to excel.\nNow, I’m performing.\nBuy my ticket.\n\nIf you have any suggestions, recommendations, or corrections please reach out to me."
  },
  {
    "objectID": "posts/failing-learning-ai/2022-08-07-fail-learning-ai-post.html#why-another-blog-post",
    "href": "posts/failing-learning-ai/2022-08-07-fail-learning-ai-post.html#why-another-blog-post",
    "title": "Learning AI And Failing Miserably",
    "section": "Why another blog post?",
    "text": "Why another blog post?\nI told you, you had the wrong guy. Anyway, I’ll try to answer.\nInternet is a fantastic place, you are plugged into a reality where you can learn whatever you want, whenever you want and at you pace.\nWhat I didn’t find yet - on web2.0 - is how to fail. How to fail programmatically, periodically and frequently. As you can image, I failed a lot and hope to keep failing at high rate.\nFor me, failing is the essence of learning, the essence of success. The Lhumann’s indicator 1 of productivity was permanent notes produced per day. The Bottoni’s indicator 2 of success is a simple counter of failings: greater the better.\nEverything started long time ago, maybe 6 years ago. I was trying to learning Machine Learning the old way: 1. collected all theoretical books; 2. collected all theoretical courses; 3. started the journey; 4. felt uncomfortable; 5. stopped; 6. and repeat the process with a slightly different theoretical book and course.\nThat’s guided me here.\n5 months ago I restarted again. The approach to Machine Learning was gradually different and this seems to be very efficient and effective.\nI followed FastAI course. In one week I completed all lectures and developed a simple system: - a box classifier which identified the kind of box with an accuracy of 97%; - a web app, which took a picture, sent it to the model and showed the result; - everything deployed in test environment.\nHope to show it as soon as my current company will make it possible.\nIt was funny, I accomplished more in that week then in 6 years of try and fail. But everything died again.\nI got overwhelmed at work. It pushed me to say goodbye to Machine Learning. Another KO. You know, when something really interesting you, one day you succeed. That day is not today.\nAfter other 5 months, today, I’m back to the mean. To the normality 3. I understood that life is not easy and is not linear. At the time I’m writing both my sons got Covid19, high fever, zero sleep. I understood to embrace those things, the gifts that life give to us 4.\nI’m ready to start again, from chapter 0."
  },
  {
    "objectID": "posts/failing-learning-ai/2022-08-07-fail-learning-ai-post.html#whats-my-next-step",
    "href": "posts/failing-learning-ai/2022-08-07-fail-learning-ai-post.html#whats-my-next-step",
    "title": "Learning AI And Failing Miserably",
    "section": "What’s my next step?",
    "text": "What’s my next step?\nI restarted ML journey and I’m happy.\nDuring those years full of failures, I sharpened my skills, where I improved a lot, I think. From note taking to programming. From learning to memorizing. All is summarized in: be more dynamic and less static.\nLet me pretend, for a moment, that I will accomplish the following steps.\nTo be able to mastering Machine Learning with FastAI approach I need to be a practitioner. To be a practitioner a have to do lots practice. To do lots practice a have to spend extra hours. To be able to spend extra hours I have to be motivated. To be motivated I have to be persistent. To be persistent I have to enjoy it.\nThat’s the chaining rule of the heaven.\n\nI listen to everything on 1x speed. I admire all you folks that do 1.5x or 2x, but my brain needs time to process the ideas. Sometimes I pause the podcast/audiobook & just reflect on a single sentence for a while. I used to think I need to improve this but now I just accept it.\n\nI’m happy to read this from Lex Fridman. It means I have to watch Jeremy at 0.5x divided by few tens speed.\nThe approach to learning, as said before, seems working. So that’s what I’ll do: 1. Review last notes, if there’s something 2. Watch lecture, first iteration. All in one breath 3. Watch lecture, second iteration. Pausing as needed, experimenting, exploring and building (lots of -ing) 4. Going through FastAI book, kaggle notes and random links 5 5. Do questionnaire and find weakness 6. Focus on a project, only one. Do it well\nThat’s wonderful approach doesn’t come from my neurons but are suggested by Radek Osmulski’s book and Jeremy Howard tips. Yes, it’s strange, I bought a book. I’m not used to.\nAdditional and personal trick to force me studying is live streaming my session study. Embarrassing and funny moment will face.\nDeadline: end of August 2022.\nI can infinitely try to learn ML and get a finite result, as per Zeno’s Paradox.\nI’ll keep you updated.\nNow let’s start. Again."
  },
  {
    "objectID": "posts/failing-learning-ai/2022-08-07-fail-learning-ai-post.html#footnotes",
    "href": "posts/failing-learning-ai/2022-08-07-fail-learning-ai-post.html#footnotes",
    "title": "Learning AI And Failing Miserably",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLuhmann wrote prolifically, with more than 70 books and nearly 400 scholarly articles published on a variety of subjects↩︎\nMe and I wrote only this blog post↩︎\nStandard deviation equals to zero↩︎\nScientists defined it as Entropy↩︎\nAlso known as syntopic reading↩︎"
  },
  {
    "objectID": "posts/2022-12-13-fallen-empire.html",
    "href": "posts/2022-12-13-fallen-empire.html",
    "title": "Quote from Karpathy",
    "section": "",
    "text": "Karpathy, 2022"
  },
  {
    "objectID": "posts/2022-12-13-fallen-empire.html#reference",
    "href": "posts/2022-12-13-fallen-empire.html#reference",
    "title": "Quote from Karpathy",
    "section": "",
    "text": "Karpathy, 2022"
  }
]