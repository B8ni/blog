{"title":"How Random Forest Can Empower A Plant","markdown":{"yaml":{"toc":true,"layout":"post","description":"Practical use case of Random Forest in Industrial environment.","author":"Francesco Bottoni","date":"10/26/2022","categories":["fastai"],"title":"How Random Forest Can Empower A Plant"},"headingText":"Preamble","containsRefs":false,"markdown":"\n\n![](forrest-gamp.png \"Forrest Gump in a Random Forest\")\n\nWhile the entire world is totally captured by Stable Diffusion, I'm experimenting **randomly into the forest of Random Forest**. Here my 2 cents after about 60+ hours of fighting against Random Forest. Actually [Forrest](https://en.wikipedia.org/wiki/Forrest_Gump) is winning the game.\n\n### Why Predicting Scraps Boxes Weight?\nIt's hard to fight entropy in my home. \n\nIt's exponentially hard to fight entropy in a plant, in an Aluminium plant precisely.\n\nThe factory would gain lots of benefits when scraps are segregated, weighted and labeled in the right way[^1].\n**Better the process and higher the impact on the revenue of the company** (true story).\n\nThe weighting process is simple: take the box, put it on an industrial scale, get the weight and repeat.\n\nThe weighting process, although heavily based on an inductive flow, it's not enough. The operators have still room of errors.\nHow can I solve this problem, without spending lot's of money? Wrong answers only: **Random Forest is the answer**. \n\nI concluded that scraps box belongs to a specific set of weights: from 0 (no box) up to 1010 KG. Simply a set of ``10`` boxes. It brings to a problem to solve: **classification problem**.\n\nStructured data + classification problem = ``RandomForestClassifier``.\n\n![](scrap_box.jpg \"Example of Aluminium Scrap Box\")\n\n## First Round\n\n|Model   |Dataset   |\n|---|---|\n|Random Forest Classifier   |CSV file, 82k rows and 33 columns.\n\nThe dataset has been created by joying most interesting tables, adding intentionally duplicated or closely correlated columns. May increase the noise or may drive to a better prediction?\n\n### Preprocessing\nSince I already worked with this data, I found a subtle feature, which is a calculated field where would create lots of troubles in production environment.\n``net_weight`` is accused of Data Leakage[^2][^3] so I firstly dropped it.\n\nObviously, Data Leakage is an issue faced later on experimentation but, IMHO, earlier you find and better it is. It's mean you have a good understanding of data.\n\nVia ``Categorify``, ``FillMissing``, ``cont_cat_split`` and ``RandomSplitter`` functions, the data is ready to be fitted.\n\n```python\nfrom fastai.tabular.all import Categorify, FillMissing, cont_cat_split, RandomSplitter\ndep = \"tare_weight\"\ndf = df.drop(\"net_weight\", axis=1)\nprocs = [Categorify, FillMissing]\n```\n\n```python\ncont,cat = cont_cat_split(df, 1, dep_var=dep)\n```\n\n```python\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(df)\n```\n\nI think ``cont_cat_split`` is a nice function to spend few seconds with. Going to [its source code](https://github.com/fastai/fastai/blob/master/fastai/tabular/core.py#L84), I can see how genuinely the continuous and categorical variables are managed:\n```python\ndef cont_cat_split(df, max_card=20, dep_var=None):\n\t\"Helper function that returns column names of cont and cat variables from given `df`.\"\n\tcont_names, cat_names = [], []\n\tfor label in df:\n\t\tif label in L(dep_var): continue\n\t\tif ((pd.api.types.is_integer_dtype(df[label].dtype) and\n\t\t\tdf[label].unique().shape[0] > max_card) or\n\t\t\tpd.api.types.is_float_dtype(df[label].dtype)):\n\t\t\tcont_names.append(label)\n\t\telse: cat_names.append(label)\n\treturn cont_names, cat_names\n```\nFor every column in dataframe, if it has more then 20 elements or it's a float, appends to continuous, otherwise categorical. I've no experience with FastAI API, but I suppose the library is full of such elegant and simple way to manage complex data and task.\n\nOnce pre-processing step is completed, the dataframe is wrapped into a ``TabularPandas``.\n``TabularPandas`` is an object. It's a simple ``DataFrame`` wrapper with transforms.\nTransforms are functions which organize the data in an optimal format. \n\n>Machine learning models are only as good as the data that is used to train them.\n\nBetter data format, better generalization.\n```python\nfrom fastai.tabular.all import TabularPandas \nto = TabularPandas(\n    df, procs, cat, cont, \n    y_names=dep, splits=splits)\n```\n```python\nto.train.xs.iloc[:3]\n```\n![](Pasted image 20221024142446.png)\n\nNow, save and train.\n```python\nfrom fastai.tabular.all import save_pickle\nsave_pickle('to.pkl',to)\n```\n\n### Fitting\n\nJeremy has developed a function which wraps ``RandomForestClassifier``. It turns useful later to apply some tuning.\n\n```python\nfrom fastai.tabular.all import load_pickle\nload_pickle('to.pkl', to)\n```\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef rf(xs, y, n_estimators=100,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n```\n```python\nm = rf(xs, y)\n```\nA good error metrics, to understand what's going on, is a simple ``mean_absolute_error``:\n```python \nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(m.predict(xs), y), mean_absolute_error(m.predict(valid_xs), valid_y)\n```\n![](Pasted image 20221024142733.png)\n\nWhat's ``mean_absolute_error``? Going to the [source code of scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/metrics/_regression.py#L141), I found line which calculate MAE:\n```python\nnp.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)\n```\nIt means:\n1. calculate the delta between ``(y_pred - y_true)``\n2. take the absolute value ``np.abs`` of the whole rows ``axis=0``\n3. finally calculate the average with ``np.average`` function\n\nNothing to add, simple enough.\n\n### Out Of Bag Error\n\nIn addition to ``mean_absolute_error``s, I have to keep an eye on ``oob_score_`` attribute which returns the **accuracy of predictions** on the **residual rows scrapped during the training**.\nObviously higher the score better the generalization on validation set.\n\n```python\nm.oob_score_\n```\n![](Pasted image 20221025160903.png)\n\nThere's so much resources where explain acutely and precisely what the hell OOB is. I'm not the right person to do that. To simplify the definition I've impressed in my mind the following raccomandation by Jeremy:\n> My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were not selected for that tree's training.\n\n### Intermediate Result\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|**1**  |**``0.709``**  | **``47.06``**|**``73.49``**|\n\n``47.06`` and ``73.49`` are just numbers. But what does it mean?\nI have achieved, via a simple ``RandomForestClassifier`` with ``100`` trees (n_estimators), an average of:\n- ``47.06`` KG of error on training set\n- ``73.49`` KG of error on validation set\n\nAnd an accuracy of ``0.709`` on the residual data not included in the fitting step.\n\nIt's clear there are multiple goals to try to achieve. A good trade off could be the following chain:\n``small_enough_error > stability > maintainability``\n\nThe next steps I'm going to walk, aims to improve the above chain.\n\n## Second Round\n\n``RandomForest`` is composed by multiples ``DecisionTrees``. \n``DecisionTrees`` are highly interpretable, so it's time to investigate the data:\n1. analyzing the most important columns, AKA ``feature_importances_``\n2. analyzing the prediction behavior for each row, AKA ``treeinterpreter``\n3. finding redundant columns, AKA ``cluster_columns``\n4. analyzing prediction confidence of the model, AKA ``std`` of each tree prediction\n5. analyzing the relationship between independent variables and dependent variable, AKA ``partial_dependece``\n6. finding out of domain data, AKA extrapolation problem\n7. analyzing where most wrong predictions go, AKA ``confusion_matrix``\n\n### Feature Importances\n\n```python\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n``` \n```python\nfi = rf_feat_importance(m, xs)\nfi[:5]\n```\n![](Pasted image 20221024145132.png)\n\nAccording to the above table:\n- the box weight prediction is mainly influenced by ``weight`` itself[^4]. Sounds reasonable;\n- ``id_machine``, in other words the machine which generates scraps, is the second most important indicator of box weight prediction. Sounds reasonable as well;\n- ``id_machine_article_description`` is the combination between ``id_machine``, ``article`` and ``description_machine``, where ``article`` is the thickness range of scarps (Ex.: from 0.5mm to 0.25mm);\n- percentage of ``id`` and ``timestamp`` is too similar. Maybe, periodically, I can expect a specific type of scraps? \n- ``code_machine`` is the short name of machine;\n- ``last_name``, the operator, contributes to the box weight prediction as well. Maybe some operators are more diligent then others?\n- ``description_machine`` is extended name of machine;\n\nEverything sounds reasonable so it seems I've discovered nothing so useful.\n\nLet's visualize ``feature_importances_`` columns.\n```python\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n```\n![](Pasted image 20221024161156.png)\n\nNow let's remove from training and validation sets features which tend to ``0`` .\n```python\nfi = fi[fi[\"imp\"] < 0.002]\n\nfiltered_xs = xs.drop(fi[\"cols\"], axis=1)\nfiltered_valid_xs = valid_xs.drop(fi[\"cols\"], axis=1)\n```\nThen fitting again the model and check the error rate (``mean_absolute_error`` and ``oob_score_``).\n```python\nm = rf(filtered_xs, y)\n```\n```python\nmean_absolute_error(m.predict(filtered_xs), y), mean_absolute_error(m.predict(filtered_valid_xs), valid_y)\n```\n![](Pasted image 20221024161755.png)\n\n```python\nm.oob_score_\n```\n![](Pasted image 20221025161630.png)\n\nHas been achieved few improvements:\n- ``mean_absolute_error`` on training set is smaller: from ``47.06`` to ``46.78``;\n- ``mean_absolute_error`` on validation set is smaller: from ``73.49`` to ``73.47``;\n- ``oob_score_`` stable: from ``0.709`` to ``0.708``\n- features reduced: from 33 to 25.\n\nNow, let's hunt redundant features.\n\n### Redundant Features\n```python\n# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ndef corr_filter(x: pd.DataFrame, bound: float):\n    corr = x.corr()\n    x_filtered = corr[((corr >= bound) | (corr <= -bound)) & (corr !=1.000)]\n    x_flattened = x_flattened.unstack().sort_values().drop_duplicates()\n    return x_flattened\n\ncorr_filter(filtered_xs, .8)\n```\n![](Pasted image 20221025101554.png)\nGiving a threshold of ``0.8``, function will return set of elements highly correlated with a score from ``0.8`` to ``0.9999``.\n\nFor a better understanding, worth to visualize them.\n```python\n# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nxs_corr = filtered_xs.corr()\ncompressed_xs = xs_corr[((xs_corr >= .5) | (xs_corr <= -.5)) & (xs_corr !=1.000)]\nplt.figure(figsize=(30,10))\nsn.heatmap(compressed_xs, annot=True, cmap=\"Reds\")\nplt.show()\n```\n![](Pasted image 20221024163716.png)\n\nAn alternative to ``heatmap`` is the helper function ``cluster_columns`` which implement a ``dendrogram`` chart.\n```python\n# https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n```\n\nNow, iteratively remove every closely correlated feature and calculate ``oob_score_``. This task is performed by ``get_oob`` function:\n```python\ndef get_oob(df):\n    m = RandomForestClassifier(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n```\n```python\nget_oob(filtered_xs)\n```\n![](Pasted image 20221025162507.png)\n```python\nto_drop = [\"id\", \"timestamp\", \"slim_alloy\", \"id_alloy\", \"pairing_alloy\",\n           \"international_alloy\", \"id_user\", \"address\",\n           \"location_name\", \"article_min_tickness\", \"article_max_tickness_na\"]\n```\n```python\n{c:get_oob(filtered_xs.drop(c, axis=1)) for c in to_drop}\n```\n![](Pasted image 20221025162546.png)\n\nGoing to remove only features with higher score.\n\n```python\nto_drop = [\"timestamp\", \"id_alloy\", \"id_user\", \"address\", \"article_min_tickness\"]\nfiltered_xs = filtered_xs.drop(to_drop, axis=1)\nfiltered_valid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n```\n```python\nm = rf(filtered_xs, y)\n```\n```python\nmean_absolute_error(m.predict(filtered_xs), y), \nmean_absolute_error(m.predict(filtered_valid_xs), valid_y)\n```\n![](Pasted image 20221025104016.png)\n```python\nm.oob_score_\n```\n![](Pasted image 20221025164058.png)\n\n### Intermediate Result\n\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|1|``0.709``  | ``47.06``|`73.49`|\n|**2**|**``0.708``**   |**`49.37`**|**``74.09``**|\n\n\nNot much worse than the model with all the fields. I've reduced some more columns (from ``25`` to ``20``) and kept stable ``oob_score_``.\nRemoving redundant features help to prevent **overfitting**.\n\n## Third Round\n\nAs showed by Jeremy, Random Forest can suffer of Extrapolation problem (:open_mouth:).\n![](Pasted image 20221025172423.png)\n\nIt means, in this case, predictions are too low with new data.\n\n> Remember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low.\n\nFor this reason I've to make sure validation set does not contain **out-of-domain data**.\n\n### Out-of-Domain Data\n\nHow to understand if the data is distributed quite properly on training set and validation set?\n```python\ndf_dom = pd.concat([filtered_xs, filtered_valid_xs])\nis_valid = np.array([0]*len(filtered_xs) + [1]*len(filtered_valid_xs))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:15]\n```\n![](Pasted image 20221026103311.png)\n\nNow, for each feature which vary a lot from training set and validation set, try to drop and check ``mean_absolute_error``. Finally, select those that keep improving the model.\n\n```python\nprint('orig', mean_absolute_error(m.predict(filtered_valid_xs), valid_y))\n\nfor c in ('id','weight', 'international_alloy', 'slim_alloy',\n          'pairing_alloy', 'id_machine_article_description', 'location_name', \"last_name\"):\n    m = rf(filtered_xs.drop(c,axis=1), y)\n    print(c, mean_absolute_error(m.predict(filtered_valid_xs.drop(c,axis=1)), valid_y))\n```\n![](Pasted image 20221026104401.png)\n\nLet's drop only ``slim_alloy``.\n```python\nto_drop = ['slim_alloy']\n\nxs_final = filtered_xs.drop(to_drop, axis=1)\nvalid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n\nm = rf(xs_final, y)\nmean_absolute_error(m.predict(valid_xs), valid_y)\n```\n![](Pasted image 20221026104546.png)\n\nKeep checking out of bag error:\n```python\nm.oob_score_\n```\n![](Pasted image 20221026104841.png)\n\n### Intermediate Result\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|1|``0.709``  | ``47.06``|`73.49`|\n|2|``0.708``   |`49.37`|``74.09``|\n|**3**|**``0.707``**   ||**``73.98``**|\n\nGood news, working on **out-of-domain data** has improved  ``mean_absolute_error`` and stabilize ``oob_score_``:\n- from ``74.09`` KG to ``73.98`` KG, validation set;\n- from ``0.708`` to ``0.707``, ``oob_score_``.\n\nWhat I have achieved so far are only small improvements. Looking at a simple chart which plots the delta between real value and prediction, I can see there's still lot of room of improvement.\n![](Pasted image 20221026110706.png)\n\nSome datapoints are consistently predicted wrong (dots at about ``-900/-1000`` and about ``900/1000``). Other visual tools like [Confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html?highlight=confusion+matrix) , **prediction confidence**, [treeinterpreter](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/) can help to analyze those behaviors.  \n\n## Final Round\nBefore any hyper-mega-super-giga tuning, my last attempt is to remove older data.  \n\nWhy? The application which manages the weighting/labeling process of scraps[^1] has been released about 2 years ago. Wouldn't surprise me if I found some strange data points, especially during first period of usage where operators were not comfortable yet with the system.\n\nRe-processing whole steps removing older 12k datapoints, seems to have generated a better baseline.\n![](Pasted image 20221026121947.png)\n\nThere's still miss-classification at around ``-900/-1000`` and ``900/1000``, it's worth investigating. However it's evident has been achieved an improvement.\n\n### Result\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|1|``0.709``  | ``47.06``|`73.49`|\n|2|``0.708``   |`49.37`|``74.09``|\n|3|``0.707``   ||``73.98``|\n|**4**|**``0.714``**   ||**``72.17``**|\n\nI think as baseline model is good entry level: fast to fit, easily interpretable and quite stable.\n\n**What I want to point out is that this small experimentation has been possible with few KBs of data, a laptop and a mediocre baseline model. An empowered model version, will avoid to spend several thousand dollars on revamping of machines!**\n\n## What's Next?\n\nOnce created a baseline model on simplified dataset, it's time to make a decision: \n- creating a NN model\n- or working on [Radom Forest tuning](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n- or switching to XGBoost model \n\n> ...roughly 80% of consequences come from 20% of causes...\n\nAs per [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle), it means, for me, to try to leverage and get as good result as soon as possible while keeping at the minimum the effort.\n\nSo next steps:\n- I will implement a Neural Network model\n- then I'll combine NN with Random Forest\n- the ensembles will work in parallel with a Computer Vision model which will try to classify the same problem (a box of scraps). \n\nAll those stuffs are aimed to develop a system where departments are notified every time the prediction of models are too different from what's happening during the weighting process. \n\nI've in mind already the application name: **Box ClassifAI**.\n\n**Keep the scraps errors lower and push higher the revenue.  That's it**.\n\n## Open Points\n- What happens if I play with categorical and continuous variables? Can them affect the prediction?\n- Plotting ``dendogram`` and removing most correlated columns. Does it change the prediction? Are columns the same?\n- Why is confidence of prediction totally wrong when the deviation reach value ``100``? Why is prediction not so bad with greater value? \n- Partial dependency plots for multi-class-classifiers?\n- Could improve Random Forest model with additional information like weather data?\n- What's happen if I convert the problem into a regression one? May the result improve?\n\n\n\n**If you have any suggestions, recommendations, or corrections please [reach out to me](https://twitter.com/bot_fra).**\n\n---\n\n[^1]: We have developed some tools to speed up and managing the process of the Aluminium scarps weighting\n[^2]: A gentle introduction to Data Leakage can be found on [Kaggle course](https://www.kaggle.com/code/alexisbcook/data-leakage)\n[^3]: A formal introduction to Data Leakage can be found on [Leakage in Data Mining paper](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf)\n[^4]: Weight - Box Weight = Net Weight\n[^5]: A not so far and futuristic classifier model which defines the right weight of box\n","srcMarkdownNoYaml":"\n\n![](forrest-gamp.png \"Forrest Gump in a Random Forest\")\n\n## Preamble\nWhile the entire world is totally captured by Stable Diffusion, I'm experimenting **randomly into the forest of Random Forest**. Here my 2 cents after about 60+ hours of fighting against Random Forest. Actually [Forrest](https://en.wikipedia.org/wiki/Forrest_Gump) is winning the game.\n\n### Why Predicting Scraps Boxes Weight?\nIt's hard to fight entropy in my home. \n\nIt's exponentially hard to fight entropy in a plant, in an Aluminium plant precisely.\n\nThe factory would gain lots of benefits when scraps are segregated, weighted and labeled in the right way[^1].\n**Better the process and higher the impact on the revenue of the company** (true story).\n\nThe weighting process is simple: take the box, put it on an industrial scale, get the weight and repeat.\n\nThe weighting process, although heavily based on an inductive flow, it's not enough. The operators have still room of errors.\nHow can I solve this problem, without spending lot's of money? Wrong answers only: **Random Forest is the answer**. \n\nI concluded that scraps box belongs to a specific set of weights: from 0 (no box) up to 1010 KG. Simply a set of ``10`` boxes. It brings to a problem to solve: **classification problem**.\n\nStructured data + classification problem = ``RandomForestClassifier``.\n\n![](scrap_box.jpg \"Example of Aluminium Scrap Box\")\n\n## First Round\n\n|Model   |Dataset   |\n|---|---|\n|Random Forest Classifier   |CSV file, 82k rows and 33 columns.\n\nThe dataset has been created by joying most interesting tables, adding intentionally duplicated or closely correlated columns. May increase the noise or may drive to a better prediction?\n\n### Preprocessing\nSince I already worked with this data, I found a subtle feature, which is a calculated field where would create lots of troubles in production environment.\n``net_weight`` is accused of Data Leakage[^2][^3] so I firstly dropped it.\n\nObviously, Data Leakage is an issue faced later on experimentation but, IMHO, earlier you find and better it is. It's mean you have a good understanding of data.\n\nVia ``Categorify``, ``FillMissing``, ``cont_cat_split`` and ``RandomSplitter`` functions, the data is ready to be fitted.\n\n```python\nfrom fastai.tabular.all import Categorify, FillMissing, cont_cat_split, RandomSplitter\ndep = \"tare_weight\"\ndf = df.drop(\"net_weight\", axis=1)\nprocs = [Categorify, FillMissing]\n```\n\n```python\ncont,cat = cont_cat_split(df, 1, dep_var=dep)\n```\n\n```python\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(df)\n```\n\nI think ``cont_cat_split`` is a nice function to spend few seconds with. Going to [its source code](https://github.com/fastai/fastai/blob/master/fastai/tabular/core.py#L84), I can see how genuinely the continuous and categorical variables are managed:\n```python\ndef cont_cat_split(df, max_card=20, dep_var=None):\n\t\"Helper function that returns column names of cont and cat variables from given `df`.\"\n\tcont_names, cat_names = [], []\n\tfor label in df:\n\t\tif label in L(dep_var): continue\n\t\tif ((pd.api.types.is_integer_dtype(df[label].dtype) and\n\t\t\tdf[label].unique().shape[0] > max_card) or\n\t\t\tpd.api.types.is_float_dtype(df[label].dtype)):\n\t\t\tcont_names.append(label)\n\t\telse: cat_names.append(label)\n\treturn cont_names, cat_names\n```\nFor every column in dataframe, if it has more then 20 elements or it's a float, appends to continuous, otherwise categorical. I've no experience with FastAI API, but I suppose the library is full of such elegant and simple way to manage complex data and task.\n\nOnce pre-processing step is completed, the dataframe is wrapped into a ``TabularPandas``.\n``TabularPandas`` is an object. It's a simple ``DataFrame`` wrapper with transforms.\nTransforms are functions which organize the data in an optimal format. \n\n>Machine learning models are only as good as the data that is used to train them.\n\nBetter data format, better generalization.\n```python\nfrom fastai.tabular.all import TabularPandas \nto = TabularPandas(\n    df, procs, cat, cont, \n    y_names=dep, splits=splits)\n```\n```python\nto.train.xs.iloc[:3]\n```\n![](Pasted image 20221024142446.png)\n\nNow, save and train.\n```python\nfrom fastai.tabular.all import save_pickle\nsave_pickle('to.pkl',to)\n```\n\n### Fitting\n\nJeremy has developed a function which wraps ``RandomForestClassifier``. It turns useful later to apply some tuning.\n\n```python\nfrom fastai.tabular.all import load_pickle\nload_pickle('to.pkl', to)\n```\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef rf(xs, y, n_estimators=100,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n```\n```python\nm = rf(xs, y)\n```\nA good error metrics, to understand what's going on, is a simple ``mean_absolute_error``:\n```python \nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(m.predict(xs), y), mean_absolute_error(m.predict(valid_xs), valid_y)\n```\n![](Pasted image 20221024142733.png)\n\nWhat's ``mean_absolute_error``? Going to the [source code of scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/metrics/_regression.py#L141), I found line which calculate MAE:\n```python\nnp.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)\n```\nIt means:\n1. calculate the delta between ``(y_pred - y_true)``\n2. take the absolute value ``np.abs`` of the whole rows ``axis=0``\n3. finally calculate the average with ``np.average`` function\n\nNothing to add, simple enough.\n\n### Out Of Bag Error\n\nIn addition to ``mean_absolute_error``s, I have to keep an eye on ``oob_score_`` attribute which returns the **accuracy of predictions** on the **residual rows scrapped during the training**.\nObviously higher the score better the generalization on validation set.\n\n```python\nm.oob_score_\n```\n![](Pasted image 20221025160903.png)\n\nThere's so much resources where explain acutely and precisely what the hell OOB is. I'm not the right person to do that. To simplify the definition I've impressed in my mind the following raccomandation by Jeremy:\n> My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were not selected for that tree's training.\n\n### Intermediate Result\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|**1**  |**``0.709``**  | **``47.06``**|**``73.49``**|\n\n``47.06`` and ``73.49`` are just numbers. But what does it mean?\nI have achieved, via a simple ``RandomForestClassifier`` with ``100`` trees (n_estimators), an average of:\n- ``47.06`` KG of error on training set\n- ``73.49`` KG of error on validation set\n\nAnd an accuracy of ``0.709`` on the residual data not included in the fitting step.\n\nIt's clear there are multiple goals to try to achieve. A good trade off could be the following chain:\n``small_enough_error > stability > maintainability``\n\nThe next steps I'm going to walk, aims to improve the above chain.\n\n## Second Round\n\n``RandomForest`` is composed by multiples ``DecisionTrees``. \n``DecisionTrees`` are highly interpretable, so it's time to investigate the data:\n1. analyzing the most important columns, AKA ``feature_importances_``\n2. analyzing the prediction behavior for each row, AKA ``treeinterpreter``\n3. finding redundant columns, AKA ``cluster_columns``\n4. analyzing prediction confidence of the model, AKA ``std`` of each tree prediction\n5. analyzing the relationship between independent variables and dependent variable, AKA ``partial_dependece``\n6. finding out of domain data, AKA extrapolation problem\n7. analyzing where most wrong predictions go, AKA ``confusion_matrix``\n\n### Feature Importances\n\n```python\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n``` \n```python\nfi = rf_feat_importance(m, xs)\nfi[:5]\n```\n![](Pasted image 20221024145132.png)\n\nAccording to the above table:\n- the box weight prediction is mainly influenced by ``weight`` itself[^4]. Sounds reasonable;\n- ``id_machine``, in other words the machine which generates scraps, is the second most important indicator of box weight prediction. Sounds reasonable as well;\n- ``id_machine_article_description`` is the combination between ``id_machine``, ``article`` and ``description_machine``, where ``article`` is the thickness range of scarps (Ex.: from 0.5mm to 0.25mm);\n- percentage of ``id`` and ``timestamp`` is too similar. Maybe, periodically, I can expect a specific type of scraps? \n- ``code_machine`` is the short name of machine;\n- ``last_name``, the operator, contributes to the box weight prediction as well. Maybe some operators are more diligent then others?\n- ``description_machine`` is extended name of machine;\n\nEverything sounds reasonable so it seems I've discovered nothing so useful.\n\nLet's visualize ``feature_importances_`` columns.\n```python\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);\n```\n![](Pasted image 20221024161156.png)\n\nNow let's remove from training and validation sets features which tend to ``0`` .\n```python\nfi = fi[fi[\"imp\"] < 0.002]\n\nfiltered_xs = xs.drop(fi[\"cols\"], axis=1)\nfiltered_valid_xs = valid_xs.drop(fi[\"cols\"], axis=1)\n```\nThen fitting again the model and check the error rate (``mean_absolute_error`` and ``oob_score_``).\n```python\nm = rf(filtered_xs, y)\n```\n```python\nmean_absolute_error(m.predict(filtered_xs), y), mean_absolute_error(m.predict(filtered_valid_xs), valid_y)\n```\n![](Pasted image 20221024161755.png)\n\n```python\nm.oob_score_\n```\n![](Pasted image 20221025161630.png)\n\nHas been achieved few improvements:\n- ``mean_absolute_error`` on training set is smaller: from ``47.06`` to ``46.78``;\n- ``mean_absolute_error`` on validation set is smaller: from ``73.49`` to ``73.47``;\n- ``oob_score_`` stable: from ``0.709`` to ``0.708``\n- features reduced: from 33 to 25.\n\nNow, let's hunt redundant features.\n\n### Redundant Features\n```python\n# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ndef corr_filter(x: pd.DataFrame, bound: float):\n    corr = x.corr()\n    x_filtered = corr[((corr >= bound) | (corr <= -bound)) & (corr !=1.000)]\n    x_flattened = x_flattened.unstack().sort_values().drop_duplicates()\n    return x_flattened\n\ncorr_filter(filtered_xs, .8)\n```\n![](Pasted image 20221025101554.png)\nGiving a threshold of ``0.8``, function will return set of elements highly correlated with a score from ``0.8`` to ``0.9999``.\n\nFor a better understanding, worth to visualize them.\n```python\n# https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nxs_corr = filtered_xs.corr()\ncompressed_xs = xs_corr[((xs_corr >= .5) | (xs_corr <= -.5)) & (xs_corr !=1.000)]\nplt.figure(figsize=(30,10))\nsn.heatmap(compressed_xs, annot=True, cmap=\"Reds\")\nplt.show()\n```\n![](Pasted image 20221024163716.png)\n\nAn alternative to ``heatmap`` is the helper function ``cluster_columns`` which implement a ``dendrogram`` chart.\n```python\n# https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\nfrom scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n```\n\nNow, iteratively remove every closely correlated feature and calculate ``oob_score_``. This task is performed by ``get_oob`` function:\n```python\ndef get_oob(df):\n    m = RandomForestClassifier(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n```\n```python\nget_oob(filtered_xs)\n```\n![](Pasted image 20221025162507.png)\n```python\nto_drop = [\"id\", \"timestamp\", \"slim_alloy\", \"id_alloy\", \"pairing_alloy\",\n           \"international_alloy\", \"id_user\", \"address\",\n           \"location_name\", \"article_min_tickness\", \"article_max_tickness_na\"]\n```\n```python\n{c:get_oob(filtered_xs.drop(c, axis=1)) for c in to_drop}\n```\n![](Pasted image 20221025162546.png)\n\nGoing to remove only features with higher score.\n\n```python\nto_drop = [\"timestamp\", \"id_alloy\", \"id_user\", \"address\", \"article_min_tickness\"]\nfiltered_xs = filtered_xs.drop(to_drop, axis=1)\nfiltered_valid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n```\n```python\nm = rf(filtered_xs, y)\n```\n```python\nmean_absolute_error(m.predict(filtered_xs), y), \nmean_absolute_error(m.predict(filtered_valid_xs), valid_y)\n```\n![](Pasted image 20221025104016.png)\n```python\nm.oob_score_\n```\n![](Pasted image 20221025164058.png)\n\n### Intermediate Result\n\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|1|``0.709``  | ``47.06``|`73.49`|\n|**2**|**``0.708``**   |**`49.37`**|**``74.09``**|\n\n\nNot much worse than the model with all the fields. I've reduced some more columns (from ``25`` to ``20``) and kept stable ``oob_score_``.\nRemoving redundant features help to prevent **overfitting**.\n\n## Third Round\n\nAs showed by Jeremy, Random Forest can suffer of Extrapolation problem (:open_mouth:).\n![](Pasted image 20221025172423.png)\n\nIt means, in this case, predictions are too low with new data.\n\n> Remember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low.\n\nFor this reason I've to make sure validation set does not contain **out-of-domain data**.\n\n### Out-of-Domain Data\n\nHow to understand if the data is distributed quite properly on training set and validation set?\n```python\ndf_dom = pd.concat([filtered_xs, filtered_valid_xs])\nis_valid = np.array([0]*len(filtered_xs) + [1]*len(filtered_valid_xs))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:15]\n```\n![](Pasted image 20221026103311.png)\n\nNow, for each feature which vary a lot from training set and validation set, try to drop and check ``mean_absolute_error``. Finally, select those that keep improving the model.\n\n```python\nprint('orig', mean_absolute_error(m.predict(filtered_valid_xs), valid_y))\n\nfor c in ('id','weight', 'international_alloy', 'slim_alloy',\n          'pairing_alloy', 'id_machine_article_description', 'location_name', \"last_name\"):\n    m = rf(filtered_xs.drop(c,axis=1), y)\n    print(c, mean_absolute_error(m.predict(filtered_valid_xs.drop(c,axis=1)), valid_y))\n```\n![](Pasted image 20221026104401.png)\n\nLet's drop only ``slim_alloy``.\n```python\nto_drop = ['slim_alloy']\n\nxs_final = filtered_xs.drop(to_drop, axis=1)\nvalid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n\nm = rf(xs_final, y)\nmean_absolute_error(m.predict(valid_xs), valid_y)\n```\n![](Pasted image 20221026104546.png)\n\nKeep checking out of bag error:\n```python\nm.oob_score_\n```\n![](Pasted image 20221026104841.png)\n\n### Intermediate Result\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|1|``0.709``  | ``47.06``|`73.49`|\n|2|``0.708``   |`49.37`|``74.09``|\n|**3**|**``0.707``**   ||**``73.98``**|\n\nGood news, working on **out-of-domain data** has improved  ``mean_absolute_error`` and stabilize ``oob_score_``:\n- from ``74.09`` KG to ``73.98`` KG, validation set;\n- from ``0.708`` to ``0.707``, ``oob_score_``.\n\nWhat I have achieved so far are only small improvements. Looking at a simple chart which plots the delta between real value and prediction, I can see there's still lot of room of improvement.\n![](Pasted image 20221026110706.png)\n\nSome datapoints are consistently predicted wrong (dots at about ``-900/-1000`` and about ``900/1000``). Other visual tools like [Confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html?highlight=confusion+matrix) , **prediction confidence**, [treeinterpreter](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/) can help to analyze those behaviors.  \n\n## Final Round\nBefore any hyper-mega-super-giga tuning, my last attempt is to remove older data.  \n\nWhy? The application which manages the weighting/labeling process of scraps[^1] has been released about 2 years ago. Wouldn't surprise me if I found some strange data points, especially during first period of usage where operators were not comfortable yet with the system.\n\nRe-processing whole steps removing older 12k datapoints, seems to have generated a better baseline.\n![](Pasted image 20221026121947.png)\n\nThere's still miss-classification at around ``-900/-1000`` and ``900/1000``, it's worth investigating. However it's evident has been achieved an improvement.\n\n### Result\n\n|Round|OOB Score|MAE Training set |MAE Validation set|\n|---|---|---|---|\n|1|``0.709``  | ``47.06``|`73.49`|\n|2|``0.708``   |`49.37`|``74.09``|\n|3|``0.707``   ||``73.98``|\n|**4**|**``0.714``**   ||**``72.17``**|\n\nI think as baseline model is good entry level: fast to fit, easily interpretable and quite stable.\n\n**What I want to point out is that this small experimentation has been possible with few KBs of data, a laptop and a mediocre baseline model. An empowered model version, will avoid to spend several thousand dollars on revamping of machines!**\n\n## What's Next?\n\nOnce created a baseline model on simplified dataset, it's time to make a decision: \n- creating a NN model\n- or working on [Radom Forest tuning](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n- or switching to XGBoost model \n\n> ...roughly 80% of consequences come from 20% of causes...\n\nAs per [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle), it means, for me, to try to leverage and get as good result as soon as possible while keeping at the minimum the effort.\n\nSo next steps:\n- I will implement a Neural Network model\n- then I'll combine NN with Random Forest\n- the ensembles will work in parallel with a Computer Vision model which will try to classify the same problem (a box of scraps). \n\nAll those stuffs are aimed to develop a system where departments are notified every time the prediction of models are too different from what's happening during the weighting process. \n\nI've in mind already the application name: **Box ClassifAI**.\n\n**Keep the scraps errors lower and push higher the revenue.  That's it**.\n\n## Open Points\n- What happens if I play with categorical and continuous variables? Can them affect the prediction?\n- Plotting ``dendogram`` and removing most correlated columns. Does it change the prediction? Are columns the same?\n- Why is confidence of prediction totally wrong when the deviation reach value ``100``? Why is prediction not so bad with greater value? \n- Partial dependency plots for multi-class-classifiers?\n- Could improve Random Forest model with additional information like weather data?\n- What's happen if I convert the problem into a regression one? May the result improve?\n\n\n\n**If you have any suggestions, recommendations, or corrections please [reach out to me](https://twitter.com/bot_fra).**\n\n---\n\n[^1]: We have developed some tools to speed up and managing the process of the Aluminium scarps weighting\n[^2]: A gentle introduction to Data Leakage can be found on [Kaggle course](https://www.kaggle.com/code/alexisbcook/data-leakage)\n[^3]: A formal introduction to Data Leakage can be found on [Leakage in Data Mining paper](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf)\n[^4]: Weight - Box Weight = Net Weight\n[^5]: A not so far and futuristic classifier model which defines the right weight of box\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-10-26-aluminium-scraps-box-weight-random-forest-post.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"cosmo","title-block-banner":true,"layout":"post","description":"Practical use case of Random Forest in Industrial environment.","author":"Francesco Bottoni","date":"10/26/2022","categories":["fastai"],"title":"How Random Forest Can Empower A Plant"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}