{"title":"Neural Network","markdown":{"yaml":{"toc":true,"layout":"post","description":"Wikibot Neural Network.","author":"Francesco Bottoni","date":"12/12/2022","categories":["wiki","neural","network"],"title":"Neural Network"},"headingText":"One Liner Definition","containsRefs":false,"markdown":"\n\nNeural Network is a mathematical function. \n\nThis mathematical function is based on inputs and parameters.\n\nInputs and parameters are multiplied and added them up. Negative values are set to zero.\n\nThese operations are repeated untill the error of prediction is minimized.\n\n## Motivation\n\nThese 3 simple steps are the foundation of any deep learning model.\n\nImplicitally they touch most important parts of NN:\n1. Inputs and parameters are multiplied and added them up => [Matrix multiplication]();\n2. Negative values are set to zero => [Rectified Linear function]();\n3. Operations repeated untill error of prediction is minimized => [Gradient descent]() on [Loss function]().\n\nThe most complex deep learning model is built on these foundamentals. Deeply understanding them will help to breaking every complex model out.\n\n## Implementation\n\n\n```python\ndef f(x): return  3*x**2 + 2*x + 1\n```\n\n\n```python\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n```\n\n\n```python\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n```\n\n\n```python\nimport numpy as np\nimport torch\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n```\n\n\n```python\nfrom functools import partial\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)\n```\n\n\n```python\ndef mae(pred, actual): return torch.abs(pred-actual).mean()\n```\n\n\n```python\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n```\n\n\n```python\nimport torch\nabc = torch.Tensor([1.1, 1.1, 1.1])\n```\n\n\n```python\nabc.requires_grad_()\n\nloss = quad_mae(abc)\n```\n\n\n```python\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n```\n\n    step=0; loss=2.42\n    step=1; loss=2.40\n    step=2; loss=2.36\n    step=3; loss=2.30\n    step=4; loss=2.21\n    step=5; loss=2.11\n    step=6; loss=1.98\n    step=7; loss=1.85\n    step=8; loss=1.72\n    step=9; loss=1.58\n\n\n## Take Away\n\n1. [Matrix multiplication]() is the key to quickly calculate multuplication and addition of inputs and parameters.\n2. [Gradient descent]() is the tool used to understand how to minimize the loss function, since loss function composed by parameters ``abc``.\n3. [Rectified Linear function](), known as [ReLU](), is a linear function which takes input and the ouput is equals to the input. If the input is negative the output is zero. It's defined as: $f(x) = max(0, x)$\n\n### Further Work\n1. Ankify:\n    - [ ] Matrix multiplication\n    - [ ] Gradient descent\n    - [ ] ReLU\n    - [ ] End to end GD which every DL model is based on\n2. [ ] Develop a Neural Network from scratch\n\n---\n## References\n- [Neural net foundations - Jeremy Howard, 2022](https://course.fast.ai/Lessons/lesson3.html)\n- [How does a neural net really work? - Jeremy Howard, 2022](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)\n- Chatting with [ChatGPT](https://chat.openai.com/)\n","srcMarkdownNoYaml":"\n\n## One Liner Definition\nNeural Network is a mathematical function. \n\nThis mathematical function is based on inputs and parameters.\n\nInputs and parameters are multiplied and added them up. Negative values are set to zero.\n\nThese operations are repeated untill the error of prediction is minimized.\n\n## Motivation\n\nThese 3 simple steps are the foundation of any deep learning model.\n\nImplicitally they touch most important parts of NN:\n1. Inputs and parameters are multiplied and added them up => [Matrix multiplication]();\n2. Negative values are set to zero => [Rectified Linear function]();\n3. Operations repeated untill error of prediction is minimized => [Gradient descent]() on [Loss function]().\n\nThe most complex deep learning model is built on these foundamentals. Deeply understanding them will help to breaking every complex model out.\n\n## Implementation\n\n\n```python\ndef f(x): return  3*x**2 + 2*x + 1\n```\n\n\n```python\ndef quad(a, b, c, x): return a*x**2 + b*x + c\n```\n\n\n```python\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n```\n\n\n```python\nimport numpy as np\nimport torch\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n```\n\n\n```python\nfrom functools import partial\n\ndef mk_quad(a, b, c): return partial(quad, a, b, c)\n```\n\n\n```python\ndef mae(pred, actual): return torch.abs(pred-actual).mean()\n```\n\n\n```python\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n```\n\n\n```python\nimport torch\nabc = torch.Tensor([1.1, 1.1, 1.1])\n```\n\n\n```python\nabc.requires_grad_()\n\nloss = quad_mae(abc)\n```\n\n\n```python\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n```\n\n    step=0; loss=2.42\n    step=1; loss=2.40\n    step=2; loss=2.36\n    step=3; loss=2.30\n    step=4; loss=2.21\n    step=5; loss=2.11\n    step=6; loss=1.98\n    step=7; loss=1.85\n    step=8; loss=1.72\n    step=9; loss=1.58\n\n\n## Take Away\n\n1. [Matrix multiplication]() is the key to quickly calculate multuplication and addition of inputs and parameters.\n2. [Gradient descent]() is the tool used to understand how to minimize the loss function, since loss function composed by parameters ``abc``.\n3. [Rectified Linear function](), known as [ReLU](), is a linear function which takes input and the ouput is equals to the input. If the input is negative the output is zero. It's defined as: $f(x) = max(0, x)$\n\n### Further Work\n1. Ankify:\n    - [ ] Matrix multiplication\n    - [ ] Gradient descent\n    - [ ] ReLU\n    - [ ] End to end GD which every DL model is based on\n2. [ ] Develop a Neural Network from scratch\n\n---\n## References\n- [Neural net foundations - Jeremy Howard, 2022](https://course.fast.ai/Lessons/lesson3.html)\n- [How does a neural net really work? - Jeremy Howard, 2022](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)\n- Chatting with [ChatGPT](https://chat.openai.com/)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2022-12-12-neural-network.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"cosmo","fontsize":"2.0em","title-block-banner":true,"layout":"post","description":"Wikibot Neural Network.","author":"Francesco Bottoni","date":"12/12/2022","categories":["wiki","neural","network"],"title":"Neural Network"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}