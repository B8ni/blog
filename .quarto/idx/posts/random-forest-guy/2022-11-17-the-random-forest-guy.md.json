{"title":"The Random Forest Guy","markdown":{"yaml":{"toc":true,"layout":"post","description":"Aluminium Scrap Box Classification.","author":"Francesco Bottoni","date":"11/17/2022","categories":["project","random","forest","multi","class","classification"],"title":"The Random Forest Guy"},"headingText":"Scrap Box Dataset","containsRefs":false,"markdown":"\n\n\nDays passed from my [first Random Forest practical experiment](https://b8ni.github.io/bottoni/fastai/2022/10/26/aluminium-scraps-box-weight-random-forest-post.html), where I was attempting to predict the weight of an Aluminium Scarp Box.\n\nSpending days going deeper on Random Forest, here you can find a revisioned and hope improved version of the [previous one post](https://b8ni.github.io/bottoni/fastai/2022/10/26/aluminium-scraps-box-weight-random-forest-post.html).\n\n[Short learning cycle](https://youtu.be/yrtAoBr3iuQ?t=144) suggested me, gradually, what's matter the most. \n\nFigure out the metrics *properly*. \n\nSame tip and trick came from [Thakur book](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf) where he underlines, before any kind of splitting: understand the data and implement the right metric.\n\n[Target drives metric](), therefore undestanding deeply the target will return the right metric.\n\n### The Problem\n\nInitially the problem to solve included `681` classes. Now I've kept only the `11` most common. \n\n[Previously](https://b8ni.github.io/bottoni/fastai/2022/10/26/aluminium-scraps-box-weight-random-forest-post.html) I was using the wrong metric, today I switched to [AUC ROC]() metric where it's mainly used on multi class classification problem.\n\nSo, but what's the target? A multi class classification problem with imbalanced data. It took me a while but worth it.\n\nWait, imbalanced what? I don't know yet. Let's dig into unbalanced data another day.\n\n## Explore the Dataset\n\n```python\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"scraps/scrap_202210181239.csv\")\n```\n\n```python\ndf.shape\n```\n\n\n```python\ndf[\"tare_weight\"].nunique()\n```\n\n\n```python\ndf[\"tare_weight\"].value_counts().head(11)\n```\n\n\n```python\ntop_classes = df[\"tare_weight\"].value_counts().head(11)\n```\n\n\n```python\n(df.shape[0]-top_classes.sum())/df.shape[0] *100\n```\n\nIn my case I want to reduce the target spectrum. From `681` classes to `11` classes. This target reduction impacts the dataset by `4.47%` of size. `670` classes are the result of **inappropriate software usage.** I'm pretty confident the current inserts are happening mostly right.\n\n\n```python\ntop_classes[\"top_classes\"] = top_classes.index\n```\n\n\n```python\ndf = df[df['tare_weight'].isin(top_classes[\"top_classes\"])]\n```\n\n\n```python\ndf.shape\n```\n\n\n```python\n82388 - 78708\n```\n\nRemoved `3680` rows which meet the `670` surplus classes: a bit cut for a big up.\n\nLet's see features and target correlation with `pairplot` method.\n\n\n```python\nimport seaborn as sns\n# df_2 = df_2[df_2[\"weight\"] <= 3500]\nsns.pairplot(df[:50], hue=\"tare_weight\")\n```\n\nI don't see any strong linear correlation (except fews which are duplicated features). It suggests Random Forest, thanks to its ability to work [uninformative features](https://hal.archives-ouvertes.fr/hal-03723551v2/document), would take advantage of the dataset form.\n\n## Data Preprocessing\n\n\n```python\nfrom fastai.tabular.all import Categorify, FillMissing, cont_cat_split, RandomSplitter\n\ndep = \"tare_weight\"\n\ndf = df.drop(\"net_weight\", axis=1)\n\nprocs = [Categorify, FillMissing]\n```\n\n\n```python\ndf = df.rename(columns={\"max_tickness.1\": \"article_max_tickness\",\n                        \"min_tickness.1\": \"article_min_tickness\",\n                        \"max_tickness\": \"alloy_max_tickness\",\n                        \"min_tickness\": \"alloy_min_tickness\",\n                        \"name\": \"location_name\"})\n```\n\n\n```python\ncont,cat = cont_cat_split(df, 1, dep_var=dep)\n```\n\n\n```python\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(df)\n```\n\n\n```python\nfrom fastai.tabular.all import TabularPandas \nto = TabularPandas(\n    df, procs, cat, cont, \n    y_names=dep, splits=splits)\n```\n\n\n```python\nto.train.xs.iloc[:3]\n```\n\n\n```python\nlen(to.train),len(to.valid)    \n```\n\n\n```python\nfrom fastai.tabular.all import save_pickle\nsave_pickle('to.pkl',to)\n```\n\n\n```python\nfrom fastai.tabular.all import load_pickle\nto = load_pickle('to.pkl')\n```\n\n\n```python\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n```\n\n\n```python\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n\n```python\ndef ovr_rf(xs, y, n_estimators=40,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True)).fit(xs, y)\n```\n\nHere I've simply re-adapted a [Jeremy](https://course.fast.ai/Lessons/lesson6.html) [function](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb) to work with [One-Versus-Rest pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html). I'm improving my buzzy worlds man!\n\n\n```python\nm  = ovr_rf(xs, y)\n```\n\n\n```python\npred_prob = m.predict_proba(valid_xs)\n```\n\n\n```python\npred_prob\n```\n\nActually I'm not using the classic `predict()` method. `pred_prob` is an array - generated by `predict_proba()` method - which contains classes probabilities. See also [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html?highlight=onevsrest+predict_proba#sklearn.multiclass.OneVsRestClassifier.predict_proba) [source code](https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/multiclass.py#L450).\n\n### ROC Curve\n\nNow it's time to analyze our performance with a different metric: AUC ROC.\n\nFirst [encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder#sklearn.preprocessing.LabelEncoder) all classes then [binirize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html?highlight=label_binarize#sklearn.preprocessing.label_binarize) and finally plot them.\n\n\n```python\n#Lets encode target labels (y) with values between 0 and n_classes-1.\n#We will use the LabelEncoder to do this. \nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\nlabel_encoder.fit(valid_y)\ntransfomerd_valid_y=label_encoder.transform(valid_y)\nclasses=label_encoder.classes_\n```\n\n\n```python\nfrom sklearn.preprocessing import label_binarize\n#binarize the y_values\nplt.figure(figsize = (15, 10))\n\ny_test_binarized=label_binarize(valid_y,classes=np.unique(valid_y))\n\n# roc curve for classes\nfpr = {}\ntpr = {}\nthresh ={}\nroc_auc = dict()\n\nn_class = classes.shape[0]\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred_prob[:,i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # plotting    \n    plt.plot(fpr[i], tpr[i], linestyle='--', \n             label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n    \n\nplt.plot([0,1],[0,1],'b--')\nplt.xlim([0,1])\nplt.ylim([0,1.05])\nplt.title('Multiclass ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n\n```python\navg_roc_auc = pd.Series(roc_auc)\navg_roc_auc.mean()\n```\n\nAn average of `94%` of being right is really good. Only `750` box is mainly miss-classified, with a `83%`.\n\n## Feature Selection\n\nFeature selection starts from ``feature_importances_``. I've adapted [Jeremy](https://course.fast.ai/Lessons/lesson6.html) [method](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb) to work with multi class model.\n\n### Feature Importances\n\n\n```python\ndef rf_feat_importance(m, df, i):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.estimators_[i].feature_importances_}\n                       ).sort_values('imp', ascending=False)\n```\n\nEvery class have its own feature importances so I have to compress everything in array and remove the last one. I've implemented a simple `concat` and `mean`.\n\n\n```python\ndf_all = pd.DataFrame()\nfor i in range(df[\"tare_weight\"].nunique()):\n    df_all = pd.concat([df_all, rf_feat_importance(m, xs, i)])\n```\n\n\n```python\ncols = df_all[\"cols\"].sort_index().unique()\n```\n\n\n```python\ndf_all = df_all.groupby(df_all.index).mean()\n```\n\n\n```python\ndf_all[\"cols\"] = cols\ndf_all = df_all.sort_values('imp', ascending=False)\n```\n\nFinally plotting averaged feature importances of the whole classes.\n\n\n```python\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(df_all[:30]);\n```\n\nLet's remove less significant ones.\n\n\n```python\ndf_all[df_all[\"imp\"] >= 0.002]\n```\n\n\n```python\nfi = df_all[df_all[\"imp\"] < 0.002]\n\nfiltered_xs = xs.drop(fi[\"cols\"], axis=1)\nfiltered_valid_xs = valid_xs.drop(fi[\"cols\"], axis=1)\n```\n\n\n```python\nfiltered_xs.shape, filtered_valid_xs.shape\n```\n\n\n```python\nm = ovr_rf(filtered_xs, y)\n```\n\n\n```python\npred_prob = m.predict_proba(filtered_valid_xs)\n```\n\n\n```python\ndef roc_plot(classes):\n    plt.figure(figsize = (15, 10))\n\n    y_test_binarized=label_binarize(valid_y,classes=np.unique(valid_y))\n\n    # roc curve for classes\n    fpr = {}\n    tpr = {}\n    thresh ={}\n    roc_auc = dict()\n\n    n_class = classes.shape[0]\n\n    for i in range(n_class):    \n        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred_prob[:,i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # plotting    \n        plt.plot(fpr[i], tpr[i], linestyle='--', \n                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n\n\n    plt.plot([0,1],[0,1],'b--')\n    plt.xlim([0,1])\n    plt.ylim([0,1.05])\n    plt.title('Multiclass ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive rate')\n    plt.legend(loc='lower right')\n    plt.show()\n```\n\n\n```python\nroc_plot(classes)\n```\n\n\n```python\ndef avg_roc_auc(pred_prob):\n    return pd.Series(roc_auc_classes(pred_prob)).mean()  \n```\n\n\n```python\navg_roc_auc(pred_prob)\n```\n\nWith just removing the less important ones, the model has improved by few decimals.\n\n\n```python\nfrom fastai.tabular.all import save_pickle\nsave_pickle('filtered_xs.pkl',filtered_xs)\nsave_pickle('filtered_valid_xs.pkl',filtered_valid_xs)\nfiltered_xs = load_pickle('filtered_xs.pkl')\nfiltered_valid_xs = load_pickle('filtered_valid_xs.pkl')\n```\n\n### Features Correlation\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nxs_corr = filtered_xs.corr()\ncompressed_xs = xs_corr[((xs_corr >= .5) | (xs_corr <= -.5)) & (xs_corr !=1.000)]\nplt.figure(figsize=(30,10))\nsn.heatmap(compressed_xs, annot=True, cmap=\"Reds\")\nplt.show()\n```\n\n\n```python\ndef corrFilter(x: pd.DataFrame, bound: float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\n\ncorrFilter(filtered_xs, .65)\n```\n\n\n```python\ndef oob_estimators(filtered_xs):\n    m = ovr_rf(filtered_xs, y)\n    return [m.estimators_[i].oob_score_ for i in range (df[\"tare_weight\"].nunique())]\n```\n\nSince I'm working with a dataset with `11` classes, it's essential to evaluate for each class relative Out-of-Bag score. So the goal is to remove closely correlated features which keep stagnant or improve the OOB score.\n\n\n```python\noob_estimators(filtered_xs)\n```\n\n\n```python\nto_drop = [\"id\", \"timestamp\", \"slim_alloy\", \"id_alloy\", \"pairing_alloy\",\n           \"international_alloy\", \"id_user\", \"address\",\n           \"location_name\", \"article_min_tickness\", \"article_max_tickness_na\"]\n```\n\n\n```python\n{c:oob_estimators(filtered_xs.drop(c, axis=1)) for c in to_drop}\n```\n\nThe features belongs to `to_drop` list with an average of `OOB` score higher, will be dropped.\n\n\n```python\nto_drop = [\"id\", \"pairing_alloy\", \"id_alloy\",\n           \"article_max_tickness_na\", \"location_name\", \"article_max_tickness_na\"]\n```\n\n\n```python\nfiltered_xs = filtered_xs.drop(to_drop, axis=1)\nfiltered_valid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n```\n\n\n```python\nfiltered_valid_xs.shape, filtered_xs.shape\n```\n\n\n```python\nm = ovr_rf(filtered_xs, y)\n```\n\n\n```python\npred_prob = m.predict_proba(filtered_valid_xs)\n```\n\n\n```python\nroc_plot(pred_prob)\n```\n\n\n```python\navg_roc_auc(pred_prob)\n```\n\nObtaining `94.5%` `AUC ROC` score while keeping `OOB` score higher is a good achievement. Breakpoint saved.\n\n\n```python\nsave_pickle('filtered_xs.pkl',filtered_xs)\nsave_pickle('filtered_valid_xs.pkl',filtered_valid_xs)\nfiltered_xs = load_pickle('filtered_xs.pkl')\nfiltered_valid_xs = load_pickle('filtered_valid_xs.pkl')\n```\n\n## Baseline Result\n\nNow it's time to fix [Out of Domain Data]() to minimize overfitting.\n\n### Out of Domain Data\n\n\n```python\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n```\n\n\n```python\ndf_dom = pd.concat([filtered_xs, filtered_valid_xs])\nis_valid = np.array([0]*len(filtered_xs) + [1]*len(filtered_valid_xs))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:15]\n```\n\n\n```python\nfor c in ('timestamp', 'weight', 'slim_alloy', \n          'international_alloy', 'id_machine_article_description',\n          'id_idp_user', 'last_name', 'id_machine', 'slim_number',\n          'first_name', 'code_machine', 'description_machine'):\n    m = ovr_rf(filtered_xs.drop(c,axis=1), y)\n    pred_prob = m.predict_proba(filtered_valid_xs.drop(c,axis=1))\n    print(c, avg_roc_auc(pred_prob))\n```\n\n\n```python\nto_drop = ['international_alloy', 'last_name', 'id_machine', 'slim_number', 'description_machine']\n```\n\n\n```python\nxs_final = filtered_xs.drop(to_drop, axis=1)\nvalid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n```\n\n\n```python\nxs_final.shape, valid_xs.shape\n```\n\n\n```python\nm = ovr_rf(filtered_xs, y)\npred_prob = m.predict_proba(filtered_valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(m)\n```\n\nEverything ended with less features (`15`) and **higher score** both `AUC ROC` and `OOB`.\n\n\n```python\nsave_pickle('final_xs.pkl',xs_final)\nsave_pickle('final_valid_xs.pkl',valid_xs)\n```\n\n### Hyperparameter Tuning\n\nBefore the game end I'll try some hypertuning.\n\n\n```python\nxs_final = load_pickle('final_xs.pkl')\nvalid_xs = load_pickle('final_valid_xs.pkl')\n```\n\n\n```python\nm.get_params()\n```\n\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 4)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'estimator__n_estimators': n_estimators,\n               'estimator__max_features': max_features,\n               'estimator__max_depth': max_depth,\n               'estimator__min_samples_split': min_samples_split,\n               'estimator__min_samples_leaf': min_samples_leaf,\n               'estimator__bootstrap': bootstrap}\n```\n\n\n```python\nrandom_grid\n```\n\n\n```python\nfrom sklearn.model_selection import ShuffleSplit\nsp = ShuffleSplit(n_splits=2, test_size=.25, random_state=42)\n```\n\n\n```python\nrf.get_params().keys()\n```\n\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier \n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = OneVsRestClassifier(RandomForestClassifier(oob_score=True))\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = sp, verbose=2, random_state=42, n_jobs = 3)# Fit the random search model\nrf_random.fit(xs_final, y)\n```\n\n\n```python\nrf_random.best_params_\n```\n\n\n```python\nfrom sklearn.metrics import accuracy_score\nbest_model = rf_random.best_estimator_\npred_prob = best_model.predict_proba(valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(best_model)\n```\n\nNow narrowing the range and trying to gain lil decimals.\n\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\nn_estimators = [50, 100, 150]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [30, 50, 100]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10, 20]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True]# Create the random grid\nrandom_grid = {'estimator__n_estimators': n_estimators,\n               'estimator__max_features': max_features,\n               'estimator__max_depth': max_depth,\n               'estimator__min_samples_split': min_samples_split,\n               'estimator__min_samples_leaf': min_samples_leaf,\n               'estimator__bootstrap': bootstrap}\n```\n\n\n```python\nrandom_grid\n```\n\n\n```python\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = OneVsRestClassifier(RandomForestClassifier(oob_score=True))\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = sp, verbose=2, random_state=42, n_jobs = 3)# Fit the random search model\nrf_random.fit(xs_final, y)\n```\n\n\n```python\nrf_random.best_estimator_\n```\n\n\n```python\nnarrowed_model = rf_random.best_estimator_\npred_prob = narrowed_model.predict_proba(valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(narrowed_model)\n```\n\nFrom `94%` to almost `94.7%` is the final score. OOB stable on `95.3%` range.\n\n## Conclusion\n\nMiss-classifying the tare weight (Aluminium scarp box) is expensive causing **damage to the company (less revenue) and environment (re-melting Aluminium)**. \n\n**Scoring a `94.7%` of predicting right is a great baseline. Sure less scraps will be wasted.**\n\n## Further Work\n\n1. Develop service which host the model.\n2. How reacts the model if I remove duplicated rows? Do it.\n3. I know the dataset is imbalanced. Implement it. \n4. Compare the result with [deep learning tabular model](https://arxiv.org/pdf/2207.08815.pdf).\n5. Compare the result with XGBoost model.\n6. Using same method to classify Aluminium alloys.\n","srcMarkdownNoYaml":"\n\n## Scrap Box Dataset\n\nDays passed from my [first Random Forest practical experiment](https://b8ni.github.io/bottoni/fastai/2022/10/26/aluminium-scraps-box-weight-random-forest-post.html), where I was attempting to predict the weight of an Aluminium Scarp Box.\n\nSpending days going deeper on Random Forest, here you can find a revisioned and hope improved version of the [previous one post](https://b8ni.github.io/bottoni/fastai/2022/10/26/aluminium-scraps-box-weight-random-forest-post.html).\n\n[Short learning cycle](https://youtu.be/yrtAoBr3iuQ?t=144) suggested me, gradually, what's matter the most. \n\nFigure out the metrics *properly*. \n\nSame tip and trick came from [Thakur book](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf) where he underlines, before any kind of splitting: understand the data and implement the right metric.\n\n[Target drives metric](), therefore undestanding deeply the target will return the right metric.\n\n### The Problem\n\nInitially the problem to solve included `681` classes. Now I've kept only the `11` most common. \n\n[Previously](https://b8ni.github.io/bottoni/fastai/2022/10/26/aluminium-scraps-box-weight-random-forest-post.html) I was using the wrong metric, today I switched to [AUC ROC]() metric where it's mainly used on multi class classification problem.\n\nSo, but what's the target? A multi class classification problem with imbalanced data. It took me a while but worth it.\n\nWait, imbalanced what? I don't know yet. Let's dig into unbalanced data another day.\n\n## Explore the Dataset\n\n```python\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"scraps/scrap_202210181239.csv\")\n```\n\n```python\ndf.shape\n```\n\n\n```python\ndf[\"tare_weight\"].nunique()\n```\n\n\n```python\ndf[\"tare_weight\"].value_counts().head(11)\n```\n\n\n```python\ntop_classes = df[\"tare_weight\"].value_counts().head(11)\n```\n\n\n```python\n(df.shape[0]-top_classes.sum())/df.shape[0] *100\n```\n\nIn my case I want to reduce the target spectrum. From `681` classes to `11` classes. This target reduction impacts the dataset by `4.47%` of size. `670` classes are the result of **inappropriate software usage.** I'm pretty confident the current inserts are happening mostly right.\n\n\n```python\ntop_classes[\"top_classes\"] = top_classes.index\n```\n\n\n```python\ndf = df[df['tare_weight'].isin(top_classes[\"top_classes\"])]\n```\n\n\n```python\ndf.shape\n```\n\n\n```python\n82388 - 78708\n```\n\nRemoved `3680` rows which meet the `670` surplus classes: a bit cut for a big up.\n\nLet's see features and target correlation with `pairplot` method.\n\n\n```python\nimport seaborn as sns\n# df_2 = df_2[df_2[\"weight\"] <= 3500]\nsns.pairplot(df[:50], hue=\"tare_weight\")\n```\n\nI don't see any strong linear correlation (except fews which are duplicated features). It suggests Random Forest, thanks to its ability to work [uninformative features](https://hal.archives-ouvertes.fr/hal-03723551v2/document), would take advantage of the dataset form.\n\n## Data Preprocessing\n\n\n```python\nfrom fastai.tabular.all import Categorify, FillMissing, cont_cat_split, RandomSplitter\n\ndep = \"tare_weight\"\n\ndf = df.drop(\"net_weight\", axis=1)\n\nprocs = [Categorify, FillMissing]\n```\n\n\n```python\ndf = df.rename(columns={\"max_tickness.1\": \"article_max_tickness\",\n                        \"min_tickness.1\": \"article_min_tickness\",\n                        \"max_tickness\": \"alloy_max_tickness\",\n                        \"min_tickness\": \"alloy_min_tickness\",\n                        \"name\": \"location_name\"})\n```\n\n\n```python\ncont,cat = cont_cat_split(df, 1, dep_var=dep)\n```\n\n\n```python\nsplits = RandomSplitter(valid_pct=0.25, seed=42)(df)\n```\n\n\n```python\nfrom fastai.tabular.all import TabularPandas \nto = TabularPandas(\n    df, procs, cat, cont, \n    y_names=dep, splits=splits)\n```\n\n\n```python\nto.train.xs.iloc[:3]\n```\n\n\n```python\nlen(to.train),len(to.valid)    \n```\n\n\n```python\nfrom fastai.tabular.all import save_pickle\nsave_pickle('to.pkl',to)\n```\n\n\n```python\nfrom fastai.tabular.all import load_pickle\nto = load_pickle('to.pkl')\n```\n\n\n```python\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n```\n\n\n```python\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n\n```python\ndef ovr_rf(xs, y, n_estimators=40,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True)).fit(xs, y)\n```\n\nHere I've simply re-adapted a [Jeremy](https://course.fast.ai/Lessons/lesson6.html) [function](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb) to work with [One-Versus-Rest pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html). I'm improving my buzzy worlds man!\n\n\n```python\nm  = ovr_rf(xs, y)\n```\n\n\n```python\npred_prob = m.predict_proba(valid_xs)\n```\n\n\n```python\npred_prob\n```\n\nActually I'm not using the classic `predict()` method. `pred_prob` is an array - generated by `predict_proba()` method - which contains classes probabilities. See also [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html?highlight=onevsrest+predict_proba#sklearn.multiclass.OneVsRestClassifier.predict_proba) [source code](https://github.com/scikit-learn/scikit-learn/blob/f3f51f9b6/sklearn/multiclass.py#L450).\n\n### ROC Curve\n\nNow it's time to analyze our performance with a different metric: AUC ROC.\n\nFirst [encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder#sklearn.preprocessing.LabelEncoder) all classes then [binirize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html?highlight=label_binarize#sklearn.preprocessing.label_binarize) and finally plot them.\n\n\n```python\n#Lets encode target labels (y) with values between 0 and n_classes-1.\n#We will use the LabelEncoder to do this. \nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\nlabel_encoder.fit(valid_y)\ntransfomerd_valid_y=label_encoder.transform(valid_y)\nclasses=label_encoder.classes_\n```\n\n\n```python\nfrom sklearn.preprocessing import label_binarize\n#binarize the y_values\nplt.figure(figsize = (15, 10))\n\ny_test_binarized=label_binarize(valid_y,classes=np.unique(valid_y))\n\n# roc curve for classes\nfpr = {}\ntpr = {}\nthresh ={}\nroc_auc = dict()\n\nn_class = classes.shape[0]\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred_prob[:,i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # plotting    \n    plt.plot(fpr[i], tpr[i], linestyle='--', \n             label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n    \n\nplt.plot([0,1],[0,1],'b--')\nplt.xlim([0,1])\nplt.ylim([0,1.05])\nplt.title('Multiclass ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n\n```python\navg_roc_auc = pd.Series(roc_auc)\navg_roc_auc.mean()\n```\n\nAn average of `94%` of being right is really good. Only `750` box is mainly miss-classified, with a `83%`.\n\n## Feature Selection\n\nFeature selection starts from ``feature_importances_``. I've adapted [Jeremy](https://course.fast.ai/Lessons/lesson6.html) [method](https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb) to work with multi class model.\n\n### Feature Importances\n\n\n```python\ndef rf_feat_importance(m, df, i):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.estimators_[i].feature_importances_}\n                       ).sort_values('imp', ascending=False)\n```\n\nEvery class have its own feature importances so I have to compress everything in array and remove the last one. I've implemented a simple `concat` and `mean`.\n\n\n```python\ndf_all = pd.DataFrame()\nfor i in range(df[\"tare_weight\"].nunique()):\n    df_all = pd.concat([df_all, rf_feat_importance(m, xs, i)])\n```\n\n\n```python\ncols = df_all[\"cols\"].sort_index().unique()\n```\n\n\n```python\ndf_all = df_all.groupby(df_all.index).mean()\n```\n\n\n```python\ndf_all[\"cols\"] = cols\ndf_all = df_all.sort_values('imp', ascending=False)\n```\n\nFinally plotting averaged feature importances of the whole classes.\n\n\n```python\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(df_all[:30]);\n```\n\nLet's remove less significant ones.\n\n\n```python\ndf_all[df_all[\"imp\"] >= 0.002]\n```\n\n\n```python\nfi = df_all[df_all[\"imp\"] < 0.002]\n\nfiltered_xs = xs.drop(fi[\"cols\"], axis=1)\nfiltered_valid_xs = valid_xs.drop(fi[\"cols\"], axis=1)\n```\n\n\n```python\nfiltered_xs.shape, filtered_valid_xs.shape\n```\n\n\n```python\nm = ovr_rf(filtered_xs, y)\n```\n\n\n```python\npred_prob = m.predict_proba(filtered_valid_xs)\n```\n\n\n```python\ndef roc_plot(classes):\n    plt.figure(figsize = (15, 10))\n\n    y_test_binarized=label_binarize(valid_y,classes=np.unique(valid_y))\n\n    # roc curve for classes\n    fpr = {}\n    tpr = {}\n    thresh ={}\n    roc_auc = dict()\n\n    n_class = classes.shape[0]\n\n    for i in range(n_class):    \n        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_binarized[:,i], pred_prob[:,i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # plotting    \n        plt.plot(fpr[i], tpr[i], linestyle='--', \n                 label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))\n\n\n    plt.plot([0,1],[0,1],'b--')\n    plt.xlim([0,1])\n    plt.ylim([0,1.05])\n    plt.title('Multiclass ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive rate')\n    plt.legend(loc='lower right')\n    plt.show()\n```\n\n\n```python\nroc_plot(classes)\n```\n\n\n```python\ndef avg_roc_auc(pred_prob):\n    return pd.Series(roc_auc_classes(pred_prob)).mean()  \n```\n\n\n```python\navg_roc_auc(pred_prob)\n```\n\nWith just removing the less important ones, the model has improved by few decimals.\n\n\n```python\nfrom fastai.tabular.all import save_pickle\nsave_pickle('filtered_xs.pkl',filtered_xs)\nsave_pickle('filtered_valid_xs.pkl',filtered_valid_xs)\nfiltered_xs = load_pickle('filtered_xs.pkl')\nfiltered_valid_xs = load_pickle('filtered_valid_xs.pkl')\n```\n\n### Features Correlation\n\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nxs_corr = filtered_xs.corr()\ncompressed_xs = xs_corr[((xs_corr >= .5) | (xs_corr <= -.5)) & (xs_corr !=1.000)]\nplt.figure(figsize=(30,10))\nsn.heatmap(compressed_xs, annot=True, cmap=\"Reds\")\nplt.show()\n```\n\n\n```python\ndef corrFilter(x: pd.DataFrame, bound: float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\n\ncorrFilter(filtered_xs, .65)\n```\n\n\n```python\ndef oob_estimators(filtered_xs):\n    m = ovr_rf(filtered_xs, y)\n    return [m.estimators_[i].oob_score_ for i in range (df[\"tare_weight\"].nunique())]\n```\n\nSince I'm working with a dataset with `11` classes, it's essential to evaluate for each class relative Out-of-Bag score. So the goal is to remove closely correlated features which keep stagnant or improve the OOB score.\n\n\n```python\noob_estimators(filtered_xs)\n```\n\n\n```python\nto_drop = [\"id\", \"timestamp\", \"slim_alloy\", \"id_alloy\", \"pairing_alloy\",\n           \"international_alloy\", \"id_user\", \"address\",\n           \"location_name\", \"article_min_tickness\", \"article_max_tickness_na\"]\n```\n\n\n```python\n{c:oob_estimators(filtered_xs.drop(c, axis=1)) for c in to_drop}\n```\n\nThe features belongs to `to_drop` list with an average of `OOB` score higher, will be dropped.\n\n\n```python\nto_drop = [\"id\", \"pairing_alloy\", \"id_alloy\",\n           \"article_max_tickness_na\", \"location_name\", \"article_max_tickness_na\"]\n```\n\n\n```python\nfiltered_xs = filtered_xs.drop(to_drop, axis=1)\nfiltered_valid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n```\n\n\n```python\nfiltered_valid_xs.shape, filtered_xs.shape\n```\n\n\n```python\nm = ovr_rf(filtered_xs, y)\n```\n\n\n```python\npred_prob = m.predict_proba(filtered_valid_xs)\n```\n\n\n```python\nroc_plot(pred_prob)\n```\n\n\n```python\navg_roc_auc(pred_prob)\n```\n\nObtaining `94.5%` `AUC ROC` score while keeping `OOB` score higher is a good achievement. Breakpoint saved.\n\n\n```python\nsave_pickle('filtered_xs.pkl',filtered_xs)\nsave_pickle('filtered_valid_xs.pkl',filtered_valid_xs)\nfiltered_xs = load_pickle('filtered_xs.pkl')\nfiltered_valid_xs = load_pickle('filtered_valid_xs.pkl')\n```\n\n## Baseline Result\n\nNow it's time to fix [Out of Domain Data]() to minimize overfitting.\n\n### Out of Domain Data\n\n\n```python\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n```\n\n\n```python\ndf_dom = pd.concat([filtered_xs, filtered_valid_xs])\nis_valid = np.array([0]*len(filtered_xs) + [1]*len(filtered_valid_xs))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:15]\n```\n\n\n```python\nfor c in ('timestamp', 'weight', 'slim_alloy', \n          'international_alloy', 'id_machine_article_description',\n          'id_idp_user', 'last_name', 'id_machine', 'slim_number',\n          'first_name', 'code_machine', 'description_machine'):\n    m = ovr_rf(filtered_xs.drop(c,axis=1), y)\n    pred_prob = m.predict_proba(filtered_valid_xs.drop(c,axis=1))\n    print(c, avg_roc_auc(pred_prob))\n```\n\n\n```python\nto_drop = ['international_alloy', 'last_name', 'id_machine', 'slim_number', 'description_machine']\n```\n\n\n```python\nxs_final = filtered_xs.drop(to_drop, axis=1)\nvalid_xs = filtered_valid_xs.drop(to_drop, axis=1)\n```\n\n\n```python\nxs_final.shape, valid_xs.shape\n```\n\n\n```python\nm = ovr_rf(filtered_xs, y)\npred_prob = m.predict_proba(filtered_valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(m)\n```\n\nEverything ended with less features (`15`) and **higher score** both `AUC ROC` and `OOB`.\n\n\n```python\nsave_pickle('final_xs.pkl',xs_final)\nsave_pickle('final_valid_xs.pkl',valid_xs)\n```\n\n### Hyperparameter Tuning\n\nBefore the game end I'll try some hypertuning.\n\n\n```python\nxs_final = load_pickle('final_xs.pkl')\nvalid_xs = load_pickle('final_valid_xs.pkl')\n```\n\n\n```python\nm.get_params()\n```\n\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 4)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'estimator__n_estimators': n_estimators,\n               'estimator__max_features': max_features,\n               'estimator__max_depth': max_depth,\n               'estimator__min_samples_split': min_samples_split,\n               'estimator__min_samples_leaf': min_samples_leaf,\n               'estimator__bootstrap': bootstrap}\n```\n\n\n```python\nrandom_grid\n```\n\n\n```python\nfrom sklearn.model_selection import ShuffleSplit\nsp = ShuffleSplit(n_splits=2, test_size=.25, random_state=42)\n```\n\n\n```python\nrf.get_params().keys()\n```\n\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier \n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = OneVsRestClassifier(RandomForestClassifier(oob_score=True))\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = sp, verbose=2, random_state=42, n_jobs = 3)# Fit the random search model\nrf_random.fit(xs_final, y)\n```\n\n\n```python\nrf_random.best_params_\n```\n\n\n```python\nfrom sklearn.metrics import accuracy_score\nbest_model = rf_random.best_estimator_\npred_prob = best_model.predict_proba(valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(best_model)\n```\n\nNow narrowing the range and trying to gain lil decimals.\n\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\nn_estimators = [50, 100, 150]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [30, 50, 100]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10, 20]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True]# Create the random grid\nrandom_grid = {'estimator__n_estimators': n_estimators,\n               'estimator__max_features': max_features,\n               'estimator__max_depth': max_depth,\n               'estimator__min_samples_split': min_samples_split,\n               'estimator__min_samples_leaf': min_samples_leaf,\n               'estimator__bootstrap': bootstrap}\n```\n\n\n```python\nrandom_grid\n```\n\n\n```python\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = OneVsRestClassifier(RandomForestClassifier(oob_score=True))\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = sp, verbose=2, random_state=42, n_jobs = 3)# Fit the random search model\nrf_random.fit(xs_final, y)\n```\n\n\n```python\nrf_random.best_estimator_\n```\n\n\n```python\nnarrowed_model = rf_random.best_estimator_\npred_prob = narrowed_model.predict_proba(valid_xs)\navg_roc_auc(pred_prob), oob_estimators_avg(narrowed_model)\n```\n\nFrom `94%` to almost `94.7%` is the final score. OOB stable on `95.3%` range.\n\n## Conclusion\n\nMiss-classifying the tare weight (Aluminium scarp box) is expensive causing **damage to the company (less revenue) and environment (re-melting Aluminium)**. \n\n**Scoring a `94.7%` of predicting right is a great baseline. Sure less scraps will be wasted.**\n\n## Further Work\n\n1. Develop service which host the model.\n2. How reacts the model if I remove duplicated rows? Do it.\n3. I know the dataset is imbalanced. Implement it. \n4. Compare the result with [deep learning tabular model](https://arxiv.org/pdf/2207.08815.pdf).\n5. Compare the result with XGBoost model.\n6. Using same method to classify Aluminium alloys.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022-11-17-the-random-forest-guy.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"cosmo","fontsize":"2.0em","title-block-banner":true,"layout":"post","description":"Aluminium Scrap Box Classification.","author":"Francesco Bottoni","date":"11/17/2022","categories":["project","random","forest","multi","class","classification"],"title":"The Random Forest Guy"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}